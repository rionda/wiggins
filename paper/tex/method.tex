\section{The \algonamebasecaps{} Algorithm}\label{sec:method}
In this section, we study the $(\theta,c)$-OPSP for a generating process
$\sys=(\family,\pi)$ on a graph $G=(V,E)$ and present the algorithm \algoname
(and its variants) to solve it.

We start we start by assuming that we have complete knowledge of $\sys$, i.e.,
we know $\family$ and $\pi$. This strong assumption allows us to study the
theoretical properties of the cost of a function and motivates the design of
our algorithm \algoname to compute an optimal schedule. We then remove the
assumption and show how we can extend \algoname to only use a collection of
observations from $\sys$. Then we discuss how to recalibrate our algorithms when
the parameters of the process (e.g., $\pi$ or $\family$) change over time.
Finally, we show an algorithm for the MapReduce framework that allows us to
scale to large networks.

\subsection{Computing the Optimal Schedule}\label{sec:optimize}
We now first conduct a theoretical analysis of the cost function $\cost_\theta$,
and then use the results to develop \algoname, our algorithm to compute the
optimal $c$-schedule (i.e., solve the $(\theta,c)$-OPSP).

\paragraph{Analysis of the cost function}
Assume for now that we know $\sys$, i.e., we have complete knowledge $\family$,
and $\pi$. With this assumption, we can exactly compute the $\theta$-cost of a
$c$-schedule.

\begin{lemma}\label{lem:explicit}
Let $\sched$ be a $c$-schedule. Then
\begin{equation}\label{eq:explicit}
	\cost(\sched) \defeq
	\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_\theta(t')] =
	\sum_{S\in \family} \frac{\pi(S)}{1- \theta(1-\sched(S))^c},
\end{equation}
where $\sched(S) = \sum_{v\in S} \sched_v$.
\end{lemma}

\todo[All]{I reorganized the proof, so please review.}
\begin{proof}
	Let $t$ be a time step, and consider the quantity $\expect[L_\theta(t)]$. By
	definition we have
	\[
		\expect[L_\theta(t)]=\expect\left[\sum_{(t',S)\in
		N_t}\fresh_\theta(t,t',S)\right]=\expect\left[\sum_{(t',S)\in
		N_t}\theta^{t-t'}\right],
	\]
	where $N_t$ is the set of uncaught items at time $t$. Let now, for any
	$t'\le t$, $N_{t,t'}\subseteq N_t$ be the set of uncaught items in the form
	$(t',S)$. Then we can write
	\[
		\expect[L_\theta(t)]=\expect\left[\sum_{t'=0}^{t}\sum_{(t',S)\in
			N_{t,t'}}\theta^{t-t'}\right]\enspace.
	\]
	Define now, for each $S\in\family$, the random variable $X_{S,t',t}$ which
	takes the value $\theta^{t-t'}$ if $(t',S)\in N_{t,t'}$, and $0$ otherwise.
	Using the linearity of expectation, we can write:
	\begin{align}\label{eq:loadexp}
		\expect[L_\theta(t)]&=\sum_{S\in\family}\sum_{t'=0}^t\expect[X_S,t',t]\nonumber\\
		&=\sum_{S\in\family}\sum_{t'=0}^t
		\theta^{t-t'}\Pr(X_{S,t,t'}=\theta^{t-t'})\enspace.
	\end{align}

	The r.v. $X_{S,t,t'}$ takes value $\theta^{t-t'}$ if and only if the
	following two events $E_1$ and $E_2$ both take place:
	\begin{itemize*}
		\item $E_1$: the set $S\in F$ belongs to $\Sample_{t'}$, i.e., is
			generated by $\sys$ at time $t'$;
		\item $E_2$: the item $(t',S)$ is uncaught at time $t$. This is
			equivalent to say that no node $v\in S$ was probed in the time
			interval $[t',t]$.
	\end{itemize*}
	We have $\Pr(E_1)=\pi(S)$, and
	\[
		\Pr(E_2)=(1-\sched(S))^{c(t-t')}\enspace.
	\]
	The events $E_1$ and $E_2$ are independent, as the event of process of
	probing the nodes is independent from the process of generating items,
	therefore, we have
	\[
		\Pr(X_{S,t,t'}=\theta^{t-t'})=\Pr(E_1)\Pr(E_2)=\pi(S)(1-\sched(S))^{c(t-t')}\enspace.
	\]
	We can plug this quantity in the rightmost term of~\eqref{eq:loadexp} and
	write
	\begin{align}
		\expect[L_\theta(t)]&=\sum_{S\in\family}\sum_{t'=0}^t\theta^{t-t'}\pi(S)(1-\sched(S))^{c(t-t')}\nonumber\\
		&=\sum_{S\in\family}\pi(S)\sum_{t'=0}^t(\theta(1-\sched(S))^c)^{t'}\nonumber\\
		&=\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c},
	\end{align}
	where we used the fact that $\theta(1-\sched(S))^c< 1$. Hence the quantity
	$\expect[L_\theta(t)]$ does not depend on $t$, and we have
	\begin{align*}
		\cost_\theta(\sched)&=\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_\theta(t)]\\
		&=\lim_{t\rightarrow\infty}\frac{1}{t}t\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c}\\
		&=
		\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c}\enspace.
	\end{align*}
\end{proof}

We now show that $\cost_\theta(\sched)$, as expressed by the
r.h.s.~of~\eqref{eq:explicit} is a convex function over its domain
$\mathsf{S}_c$, the set of all possible $c$-schedules. We then use this result
to show how to compute an optimal schedule.

\begin{theorem}\label{thm:convexity}
	The cost function $\cost_\theta(\sched)$ is a convex function over
	$\mathsf{S}_c$.
\end{theorem}
\begin{proof}
	For any $S\in\family$, let
	\[
		f_S(\sched) = \frac{1}{1 - \theta(1-\sched(S))^c}\enspace.
	\]
	The function $\cost_\theta(\sched)$ is a linear combination of
	$f_S(\sched)$'s with positive coefficients. Hence to show that
	$\cost_\theta(\sched)$ is convex it is sufficient to show that, for any
	$S\in\family$, $f_S(\sched)$ is convex.

	We start by showing that $g_S(\sched) = \theta(1-\sched(S))^c$ is convex.
	This is because its Hessian matrix is positive semidefinite. Indeed we have:
	\[
		\frac{\partial}{\partial \sched_i \partial \sched_j} g_S(\sched)=\left\{
		\begin{array}{lr}
		\theta c(c-1) (1-\sched(S))^{c-2} &  i,j \in S\\
		0 &  \text{otherwise}
		\end{array}
		\right.
	\]
	Let $\mathbf{v}_S$ be a $n\times 1$ vector in $\mathbb{R}^n$ such that its
	$i$-th coordinate is $\left[c(c-1) (1-\sched(S))^{c-2} \right]^{1/2}$ if $i\in
	S$, and $0$ otherwise. We can write the Hessian matrix of $g_S$ as
	\[
		\nabla^2 g_S = V_S * V_S^\mathsf{T},
	\]
	and thus, $\nabla^2 g_S$ is positive semidefinite matrix and $g$ is convex.
	From here, we have that $1-g_S$ is a \emph{concave} function. Since
	$f_S(\sched)=\frac{1}{1-g_S(\sched)}$ and the function $h(x)=\frac{1}{x}$ is
	convex and non-increasing, then $f_S$ is a convex function.
\end{proof}

If for every $v\in V$, $S=\{v\}$ belongs to $\family$, then the
function $g_S$ in the above proof is \emph{strictly} convex, and so is $f_S$.

We then have the following corollary of Thm.~\ref{thm:convexity}.

\begin{corollary}\label{corol:convexity}
	Any schedule $\sched$ with locally minimum cost is an optimal schedule
	(i.e., it has global minimum cost). Furthermore, if for every $v\in V$,
	$\{v\}$ belongs to $\family$, the optimal schedule is \emph{unique}.
\end{corollary}

\paragraph{The algorithm}
Corollary~\ref{corol:convexity} implies that one can compute an optimal
$c-$schedule $p^*$ (i.e., solve the $(\theta,c)$-OPSP) by solving the
unconstrained minimization of $\cost_\theta$ over the set $\mathsf{S}_c$ of all
$c$-schedules, or equivalently by solving the following constrained minimization
problem on $\mathbb{R}^+$:
\begin{equation}\label{eq:optimization}
	\boxed{
	\begin{array}{rrclcl}
	\displaystyle \min_{\sched\in\mathbb{R}_n} & \multicolumn{3}{l}{\cost{\sched}} \\
	&\displaystyle \sum_{i=1}^{n} \sched_i & = & 1 \\
	&\sched_i & \geq & 0 & & \forall i \in \{1,\ldots,n\} \\
	\end{array} }
\end{equation}
Since the function $\cost_\theta$ is convex and the constraints are linear, the
optimal solution can, theoretically, be found efficiently~\citep{BoydV04}. In
practice though, available convex optimization problem solvers can not scale
well with the number $n$ of variables, especially when $n$ is in the millions,
as is the case for modern graphs like online social networks or the Web. Hence
we developed \algoname, an iterative method based on \emph{Lagrange
multipliers}\citemissing, which can scale efficiently and can be adapted to the
MapReduce framework of computation~\citep{dean2008mapreduce}, as we show in
Sect.~\ref{sec:mapreduce}.
\todo[Ahmad]{You mentioned something about main memory as motivation for using
	the iterative method, please add that observation here.}
While we can not prove that this iterative method
always converges, we can prove that (i) if at any iteration the algorithm
examines an optimal schedule, then it will reach convergence at the next
iteration, and (ii) if it converges to a schedule, that schedule is optimal (see
Thm.~\ref{thm:optimal}). In Sect.~\ref{sec:exp} we show our experimental results
illustrating the convergence of \algoname in different cases.

\algoname takes as inputs the collection $\family$, the function $\pi$, and the
parameters $c$ and $\theta$, and outputs a schedule $\sched$ which, if
convergence (defined in the following) has been reached, is the optimal
schedule. It starts from a uniform schedule $\sched^{(0)}$, i.e.,
$\sched^{(0)}_i=1/n$ for all $1\le i\le n$, and iteratively refines it until
convergence (or until a user-specified maximum number of iterations have been
performed). At iteration $j\ge 1$, we compute, for each value $i$, $1\le i\le
n$, the function
\begin{equation}\label{eq:functionw}
	W_i(\sched^{(j-1)}) \defeq \sum_{\substack{S \in \family \\ \mbox{s.t.} i\in
S}} \frac{\theta c \pi(S)
	(1-\sched^{(j-1)}(S))^{c-1}}{(1-\theta(1-\sched^{(j-1)}(S))^c)^2}
\end{equation}
and then set
\[
	\sched^{(j)}_i = \frac{\sched^{(j-1)}_i W_i(\sched^{(j-1)})}{\sum_{z=1}^n
	\sched^{(j-1)}_z W_z(\sched^{(j-1)})}\enspace.
\]
The algorithm then checks whether $\sched^{(j)}=\sched^{(j-1)}$. If so, then we
reached convergence and we can return $\sched^{(j)}$ in output, otherwise we
perform iteration $j+1$. The pseudocode for \algoname is in
Algorithm~\ref{alg:iterative}. The following theorem shows the correctness of
the algorithm in case of convergence.

\begin{theorem}\label{thm:optimal}
	We have that:
	\begin{enumerate*}
		\item if at any iteration $j$ the schedule $\sched^{(j)}$ is optimal,
			then \algoname reaches convergence at iteration $j+1$; and
		\item if \algoname reaches convergence, then the returned schedule
			$\sched$ is optimal.
	\end{enumerate*}
\end{theorem}

\begin{proof}
	From the method of the Lagrange multipliers~\citemissing, we have that, if a
	schedule $\sched$ is optimal, then there exists a value
	$\lambda\in\mathbb{R}$ such that $\sched$ and $\lambda$ form a solution to
	the following system of $n+1$ equations in $n+1$ unknowns:
	\begin{equation}\label{eq:lagrange}
		\nabla [\cost{\sched} + \lambda (\sched_1+\ldots+\sched_n-1)] = 0,
	\end{equation}
	where the gradient on the l.h.s.~is taken w.r.t.~(the components of)
	$\sched$ and to $\lambda$ (i.e., has $n+1$ components).

	For $1\le i\le n$, the $i$-th equation induced by~\eqref{eq:lagrange} is
	\[
		\frac{\partial}{\partial \sched_i} \cost{\sched} + \lambda = 0,
	\]
	or, equivalently,
	\begin{equation}\label{eq:lagrange_i}
		\sum_{\substack{S\in\family\\\mbox{s.t.} i\in S}} \frac{\theta c
			\pi(S) (1-\sched(S))^{c-1}}{(1-\theta(1-\sched(S))^c)^2} =
			\lambda\enspace.
	\end{equation}
	The term on the l.h.s.~is exactly $W_i(\sched)$.
	The $n+1$-th equation of the system~\eqref{eq:lagrange} (i.e., the one
	involving the partial derivative w.r.t.~$\lambda$) is
	\begin{equation}\label{eq:lagrange_l}
		\sum_{z=1}^n \sched_z = 1\enspace.
	\end{equation}

	Consider now the point 1 in the statement of the theorem, and assume that we
	are at iteration $j$ such that $j$ is the minimum iteration index for which
	the schedule $\sched^{(j)}$ computed at the end of iteration $j$ is optimal.
	Then, for any $i$, $1\le i\le n$, we have
	\[
		W_i(\sched^{(j)})=\lambda
	\]
	because $\sched^{(j)}$ is optimal and hence all identities in the form
	of~\eqref{eq:lagrange_i} must be true.  Moreover, \eqref{eq:lagrange_l} must
	also hold for $\sched^{(j)}$, so we have
	\[
		\sum_{z=1}^n \sched^{(j)}_z = 1\enspace.
	\]
	Hence, for any $1\le i\le n$, we can write the value $\sched^{(j+1)}_i$
	computed at the end of iteration $j+1$ as
	\[
		\sched^{(j+1)}_i = \frac{\sched^{(j)}_i W_i(\sched^{(j)})}{\sum_{z=1}^n
		\sched^{(j)}_z W_z(\sched^{(j)})}=
		\frac{\sched^{(j)}_i\lambda}{1\lambda}=\sched^{(j)}_i,
	\]
	which means that we reached convergence and \algoname will return
	$\sched^{(j+1)}$ in output, which is optimal because
	$\sched^{(j+1)}=\sched^{(j)}$.

	Consider now point 2 in the statement of the theorem, and let $j$ be the
	first iteration for which $\sched^{(j)}=\sched^{(j-1)}$. Then we have, for
	any $1\le i\le n$,
	\[
		\sched^{(j)}_i = \frac{\sched^{(j-1)}_i W_i(\sched^{(j-1)})}{\sum_{z=1}^n
		\sched^{(j-1)}_z W_z(\sched^{(j-1)})} = \sched^{(j-1)}_i\enspace.
	\]
	This implies
	\begin{equation}\label{eq:identityw}
		W_i(\sched^{(j-1)}) = \sum_{z=1}^n \sched^{(j-1)}_z W_z(\sched^{(j-1)})
	\end{equation}
	and the r.h.s.~does not depend on $i$, and so neither does
	$W_i(\sched^{(j-1)})$. Hence we have
	$W_1(\sched^{(j-1)})=\dotsb=W_n(\sched^{(j-1)})$ and can
	rewrite~\eqref{eq:identityw} as
	\[
		W_i(\sched^{(j-1)}) =  \sum_{z=1}^n \sched^{(j-1)}_z
		W_i(\sched^{(j-1)}),
	\]
	which implies that the identity~\eqref{eq:lagrange_l} holds for
	$\sched^{(j-1)}$. Moreover, if we set
	\[
		\lambda = W_1(\sched^{(j-1)})
	\]
	we have that all the identities in the form of~\eqref{eq:lagrange_i} hold.
	Then, $\sched^{(j-1)}$ and $\lambda$ form a solution to the
	system~\eqref{eq:lagrange}, which implies that $\sched^{(j-1)}$ is optimal
	and so must be $\sched^{(j)}$, the returned schedule, as it is equal to
	$\sched^{(j-1)}$ because \algoname reached convergence.
\end{proof}

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKwComment{tcp}{//}{}
	\SetKw{KwBreak}{break}
	\Input{$\family$, $\pi$, $c$, $\theta$, and maximum number $T$ of iterations}
	\Output{An optimal $c$-schedule $\sched$ with minimum $\theta$-cost}
	\ForEach{$i\leftarrow 1$ \KwTo $n$}{
		$\sched_i \leftarrow 1/n$\;
	}
	\For{$j\leftarrow 1$ \KwTo $T$} {
		\ForEach{$i\leftarrow 1$ \KwTo $n$}{
			$W_i\leftarrow 0$\;
		}
		\For{$S\in \family$} {
			\For{$i\in S$} {
				$W_i \leftarrow W_i + \theta c \pi(S) (1-\sched(S))^{c-1}/(1-\theta(1-\sched(S))^c)^2$\label{algline:w}\;
			}
		}
		$\sched_{\mathrm{old}}\leftarrow \sched$\;
		\ForEach{$i\leftarrow 1$ \KwTo $n$}{
			$\sched_i\leftarrow \frac{\sched_iW_i}{\sum_{i} \sched_iW_i}$\;
		}
		\If(// test for convergence){$\sched_{\mathrm{old}} = \sched$} {
			%\tcp{Reached convergence}
			\KwBreak\;
		}
	}
	\Return{$\sched$}\;
	\caption{\algoname}
	\label{alg:iterative}
\end{algorithm}
% Matteo 7/1. The following sentence is commented because we do not know whether
% is true. It is not clear what $F$ is to me. Ahmad may have an idea.
%
%``However, this iterative method is convergent if the Jacobian norm of $F$ is
%less than 1, i.e., $\|DF\| < 1$.''

\subsection{Approximation through sampling}\label{sec:sampcomp}
We now remove the assumption, not realistic in practice, of knowing the
generating process $\sys$ exactly through $\family$ and $\pi$. We assume
instead of being able to observe, for each time step $t$ in a limited time
interval $[a,b]$, the set $\Sample_t$ generated by $\sys$, and therefore to have
access to a collection
\begin{equation}\label{eq:sample}
	\Sample=\{\Sample_a,\Sample_{a+1},\dotsc,\Sample_b\}
\end{equation}
We usually refer to $\Sample$ as a \emph{sample gathered in the time interval
$[a,b]$}. Here we show that the cost of a schedule can be approximated within a
multiplicative factor $\varepsilon\in[0,1]$ if $b-a=O(\varepsilon^2\log n)$, and
that it is possible to adapt \algoname to this case.

We start by defining the cost of a schedule according to a sample.

\begin{definition}\label{def:costsample}
	Suppose $\sched$ is a $c$-schedule and $\Sample$ is as in~\eqref{eq:sample},
	with $b-a=\ell$. The $\theta$-cost of $\sched$ according to $\Sample$
	denoted by $\cost_\theta{\sched,\Sample}$ is defined as
	\[
		\cost_\theta(\sched,\Sample)\defeq\frac{1}{\ell}\sum_{S\in \Sample}
		\frac{1}{1-\theta(1-\sched(S))^c}\enspace.
	\]
\end{definition}

For $1\le i\le n$, define now the functions
\[
	W_i(\sched,\Sample) = \frac{1}{\ell(\Sample)}\sum_{S\in \Sample: i\in S}
	\frac{\theta c (1-\sched(S))^{c-1}}{(1-\theta(1-\sched(S))^c)^2}\enspace.
\]

We can then define a variant of \algoname, which we call \algonameapx, that uses
the $W_i(\sched,\Sample)$ instead of $W_i(\sched)$ (i.e., on
line~\ref{algline:w} in Alg.~\ref{alg:iterative}).

\todo[Ahmad]{I trusted you when you told me that the following is true. Please
check.}
If \algonameapx reaches convergence, it returns a schedule with the minimum cost
w.r.t.~the sample $\Sample$. More formally, by following the same steps as in
the proof of Thm.~\ref{thm:optimal}, we can prove the following result about
\algonameapx.

\begin{lemma}\label{lem:optimal_sample}
	We have that:
	\begin{enumerate*}
		\item if at any iteration $j$ the schedule $\sched^{(j)}$ has minimum
			cost w.r.t.~$\Sample$, then \algonameapx reaches convergence at
			iteration $j+1$; and
		\item if \algonameapx reaches convergence, then the returned schedule
			$\sched$ has minimum cost w.r.t.~$\Sample$.
	\end{enumerate*}
\end{lemma}

Let $\ell(\Sample)$ denote the length of the time interval during which
$\Sample$ was collected. For a $c$-schedule $\sched$,
$\cost_\theta(\sched,\Sample)$ is an approximation of $\cost_\theta(\sched)$,
and intuitively the larger $\ell$, the better the approximation.

We now show that, if $\ell(\Sample)$ is large enough, then, with high
probability (i.e., with probability at least $1-1/n^r$ for some constant $r$),
the schedule $\sched$ returned by \algonameapx in case of convergence has a cost
$\cost(\sched)$ that is close to the cost $\cost(\sched^*)$ of an optimal
schedule $\sched^*$.  Specifically, we show the following.

\begin{theorem}\label{thm:approx_sample}
	Let $r$ be a natural number, and let $\Sample$ be a sample gathered during a
	time interval of length
	\begin{equation}\label{eq:samplesize}
		\ell(\Sample) \geq
		\frac{3(r\ln(n)+\ln(4))}{\varepsilon^2(1-\theta)}\enspace.
	\end{equation}
	Let $\sched^*$ be an optimal schedule, i.e., a schedule with minimum cost
	$\cost(\sched)$. If \algonameapx converges, then the returned schedule
	$\sched$ is such that
	\[
		\cost(\sched^*)\le\cost(\sched)\le\frac{1+\varepsilon}{1-\varepsilon}\cost(\sched^*)\enspace.
	\]
\end{theorem}

Before we can prove Thm.~\ref{thm:approx_sample}, we need to show the following
technical lemma.

\begin{lemma}\label{lem:chernoffcost}
	Let $\sched$ be a $c$-schedule and $\Sample$ be a sample gathered during a
	time interval of length
	\[
		\ell(\Sample) \geq \frac{3(r\ln(n)+\ln(2))}{\varepsilon^2(1-\theta)},
	\]
	where $r$ is any natural number. Then, for every schedule $\sched$ we have
	\[
		\Pr(|\cost_\theta(\sched,\Sample) - \cost_\theta(\sched)|\geq
		\varepsilon\cdot\cost_\theta(\sched)) < \frac{1}{n^r}\enspace .
	\]
\end{lemma}

\begin{proof}
	For any $S\in\family$, let $X_S$ be a random variable which is
	$\frac{1}{1-\theta(1-\sched(S))^c}$ with probability $\pi(S)$, and zero
	otherwise. Since $\sched(S)\in [0,1]$, we have
	\[
		1\leq X_S \leq \frac{1}{1-\theta}\enspace.
	\]
	If we let $X = \sum_{S\in \family} X_S$, then
	\begin{equation}\label{eq:boundcost}
		\cost_\theta(\sched) = \expect[X] = \sum_{S\in\family} \expect[X_{S}]
		\geq \sum_{S\in\family}\pi(S)\enspace.
	\end{equation}
	Let $Z=\sum_{S\in\family}\pi(S)$. Then we have
	\[
		Z \leq X \leq \frac{Z}{1-\theta}\enspace.
	\]
	Let $X^i_S$ be the $i$-th draw of $X_S$ during the time interval $\Sample$
	was sampled from and define $X^i = \sum_{S\in\family}X_S^i$. We have
	\[
		\cost_\theta(\sched,\Sample) =
		\frac{1}{\ell(\Sample)}\sum_{i}X^i\enspace.
	\]
	Let now
	\[
		\mu=\frac{\ell(\Sample)(1-\theta)}{|Z|}\cost_\theta(\sched)\enspace.
	\]
	By using the Chernoff bound for Binomial random
	variables~\citep[Corol.~4.6]{MitzenmacherU05}, we have
	\begin{align*}
		&\Pr\left(\left|\cost_\theta(\sched,\Sample) - \cost(\sched)\right| \geq \varepsilon
		\cost_\theta(\sched)\right)\\
		&=\Pr\left(\left|\sum_i X^i - \ell(\Sample)\cost_\theta(\sched)\right| \geq \varepsilon
		\ell(\Sample) \cost(\sched)\right) \\
		&= \Pr\left(\left|\frac{1-\theta}{|\family|}\sum_i X^i - \mu\right| \geq \varepsilon
		\mu\right) \leq 2\exp\left(-\frac{\varepsilon^2\mu}{3}\right)\\
		&\le 2\exp\left(-\frac{\varepsilon^2 \ell(\Sample) (1-\theta)
			\cost(\sched)}{3|\family|}\right) \leq 2\exp\left(-\frac{\varepsilon^2 \ell(\Sample) (1-\theta)}{3}\right),
	\end{align*}
	where the last inequality follows from the rightmost inequality
	in~\eqref{eq:boundcost}. The thesis follows from our choice of
	$\ell(\Sample)$.
\end{proof}

We can now prove Thm.~\ref{thm:approx_sample}.
\begin{proof}[of Thm.~\ref{thm:approx_sample}]
	For our choice of $\ell(\Sample)$ we have, through the union bound, that,
	with probability at least $1-1/n^r$, at the same time:
	\begin{align}\label{eq:boundedcosts}
		(1-\varepsilon)\cost(\sched)&\leq \cost(\sched,\Sample) &\le
		(1+\varepsilon)\cost(\sched) & \mbox{, and}\\
		(1-\varepsilon)\cost(\sched)&\leq \cost(\sched,\Sample) &\le
		(1+\varepsilon)\cost(\sched)&\nonumber
	\end{align}
	Since we assumed that \algonameapx reached convergence when computing
	$\sched$, then Thm.~\ref{thm:approx_sample} holds, and $\sched$ is a
	schedule with minimum cost w.r.t.~$\Sample$. In particular, it must be
	\[
		\cost(\sched,\Sample)\le \cost(\sched^*,\Sample)\enspace.
	\]
	From this and~\eqref{eq:boundedcosts}, We then have
	\[
		(1-\varepsilon)\cost(\sched)\leq \cost(\sched,\Sample) \le
		\cost(\sched^*,\Sample)\le (1+\varepsilon)\cost(\sched^*)
	\]
	and by comparing the leftmost and the rightmost terms we get the thesis.
\end{proof}

% Matteo commented out on 7/1
%Using a very similar argument we have the following theorem:
%\begin{theorem}\label{thm:chernoffW}
% Suppose $\Sample$ is a sample gathered during a time interval of length
% $\ell(\Sample) \geq \frac{3(r\log(n)+\log(2))}{\varepsilon^2(1-\theta)} =
% O\left(\frac{\log(n)}{\varepsilon^2}\right)$. Then, for every schedule $\sched$ and
% $i\in\{1,\dots,n\}$ we have
% $$\Pr(|W_i(\sched,\Sample) - W_i(\sched)|\geq \varepsilon\cdot W_i(\sched)) < \frac{1}{n^r}.$$
%\end{theorem}


\subsection{Learning Dynamic Parameters}\label{sec:dynamic}
In this short section, we consider the case when the parameters of the network change (i.e, the distribution over the possible informed-sets changes).  To apply our method to a dynamic environment we first sample the process to generate a sufficiently large sample collection. This can be done by probing
 all nodes with uniform distribution during a short time interval, or using a round robin schedule (Algorithm~\ref{alg:sampler}). For an item the sampling process stores the set of nodes that received this item. We then compute an optimal schedule with respect to that sample (Algorithm~\ref{alg:iterative}). We use this schedule and monitor the cost to detect significant changes. In that case we obtain a new sample, optimize with respect to that sample and apply the new schedule.

 Note that when we adapt our schedule to the new environment (using the most recent sample) the system converges to its stable setting exponentially (in $\theta$) fast: Suppose $L$ items have been generated since the change of the parameters until we adapt the new schedule. These items, if not caught, loose their freshness exponentially fast: after $t$ steps their freshness is at most $L\theta^t$ and gets diminished very quickly.

 In our experiments we provide different examples that illustrate how load of the generating process stables after the algorithm adapts itself to the changes of parameters (see Section\ref{sec:exp}).


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[!h]
\BlankLine
{\bf Inputs:} The length of the time interval that we sample from, $\texttt{time\_length}$.

\Begin{
$\D_1 \leftarrow \emptyset$\;
$\D_2 \leftarrow \emptyset$\;
 \For{nodes $u\in U$}{
   \For{items $i$ at $u$, not older than \texttt{time\_length} time steps}{
	   $\D_1 \leftarrow \D_1 \cup \{i\}$\;
      $\D_2 \leftarrow \D_2 \cup \{(i,u)\}$\;
      }
    }


$\Sample \leftarrow \emptyset$\;
\For{$i \in D_1$}{
  $S \leftarrow \emptyset$\;
  \For{$(i, u) \in D_2$}{
  $S \leftarrow S \cup \{u\}$\;
  }
  $\Sample \leftarrow \Sample \cup \{S\}$\;
  }

  return $\Sample$\;

}

\caption{XXXX}\label{alg:sampler}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}[!h]
% \BlankLine
% {\bf Inputs:} A sample collection, $\M$, and the number of iterations, $\texttt{iter}$.
%
% \Begin{
% $\sched\leftarrow (1/n,1/n,\ldots,1/n)$\;
%  \For{$t\in\set{1,\ldots,\texttt{iter}}$}{
%   $c\leftarrow (0,\ldots,0)$\;
%    \For{$S \in \M$}{
%      \For{$i \in \Sample$}{
%       $c_i \leftarrow c_i + \paran{\frac{\sched_i}{\sum_{j\in S \sched_j}}}^2$ \;
%       }
%     }
%
%   }
% \For{$i\in \set{1,\ldots,n}$}{
%   $\sched_i \leftarrow \frac{1}{\sum_{i=1}^n c_i}(c_1, \ldots, c_n)$\;
%   }
% return $\sched$\;
% }
%
% \caption{$\optimizer(\M, \texttt{iter})$}\label{alg:optimizer}
% \end{algorithm}









%  Outline of the algorithm is given in Figure~\ref{??}.

% \subsection{Sufficient sample}
% Our goal is to compute an (approximately) optimal schedule using one sample collection
% ${\cal M}=\{S_1,\dots,S_m\}$, of $m$ informed sets generated according to the distribution $\pi$.
% Identifying an optimal function among a set of functions using one sample collection requires strong statistical properties of the sample.
% We need to show that the cost of {\em any} schedule $\sched$, considered though out the optimization process, on the sample is a close approximation of the cost of $\sched$ on the entire distribution $\pi$.
% In particular we require that for very $i=1,\dots,n$, and any schedule $\sched$, the estimate
% $${C}_i(\sched,\M)=\frac{1}{m}\sum_{i=1}^m \left( \frac{\sched_i}{\sum_{j\in S_i} \sched_j} \right )^2$$ is sufficiently good estimate for
% $${C}_i(\sched)=\sum_{S:i\in S} \pi(S) \left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2.$$
% Thus, we need a {\em uniform convergence result} of the sample with respect to this functions over all possible schedules.
%
% We use the {\em Rademacher complexity} method~\cite{shalev2014understanding} to analyze our sample:
% Given a set of functions $F$ and a set of $m$ samples $M=z_1,\dots,z_m$, the
% %Rademacher complexity $R_m(F)$ and the
% empirical Rademacher sum $R_M(F)$
% %are
% %is $$R_m(F) = E_S [E_{\bar{\sigma}} [\sup_{f \in F}  ( \frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i )]], $$
% is $$ R_M (F) = E_{\bar{\sigma}} \left[\sup_{f \in F} \paran{\frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i} \right],$$
% %respectively,
% where $\bar{\sigma}=(\sigma_1,\dots, \sigma_m)$ is a vector of $m$ independent random variables, with $Pr(\sigma_i =1)=Pr(\sigma_i = -1)=1/2$.
% %The role of the $\sigma$ variables is to create a kind of output noise in the dataset. The supremum carries out an optimization process over all functions in %$F$ trying to fit this noise. The Rademacher complexity is high if the function class is successful at fitting this noise and low otherwise.
% Like the VC-dimension (for range spaces), the Rademacher sum is a measure of the expressiveness or complexity of a set of functions $F$. We use the following relation between the Rademacher sum and the uniform convergence of a sample:
%
% \begin{theorem}~\cite[Theorem 3.1]{mohri2012foundations}
% \label{th:rc}
% Let $F$ be a (finite or measurable) set of functions on a domain $Z$ such that for all $f\in F$, $f:Z\rightarrow [0,1]$. Let  $M=\{z_1,\dots, z_m\}$ be a sample chosen from a distribution $D$ on $Z$.
% With probability $1-\delta$ over the choices of $M$,
% %over the distribution of $S$,
% simultaneously for all functions $f\in F$, we have
% %$$E_D[f(z)] \le E_S[f(z)] + 2 R_m(F) + \sqrt{\frac{\ln(1/\delta)}{2m}},$$
% %and
% %$$ E_D[f(z)] \le E_S[f(z)] + 2 R_S(F) + 3\sqrt{\frac{\ln(2/\delta)}{2m}}.$$
% \begin{eqnarray}
% \label{bound1}
% E_D[f(z)] \le \hat{E}_M [f(z)] + 2 R_M (F) + 3\sqrt{\frac{\ln(1/\delta)}{2m}},
% \end{eqnarray}
% \end{theorem}
%
% To apply this theorem to our problem we observe that
% $$\frac{{C}_i(\sched)}{ \sum_{S:i\in S} \pi(S) }=\frac{1}{ \sum_{S:i\in S} \pi(S) }\sum_{S:i\in S} \pi(S) \left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2$$
% is the expectation of the function $f^i_{\sched}(S)=\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2$ over the distribution $D_{|i}$ (the distribution of sets that include node $i$). To bound the Rademacher empirical sum of the set of functions
% $$F=\left \{f^i_{\sched}(S)=\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2~\big |~1\leq i\leq n,~\sched\in [0,1]^n, ~\sum_{i=1}^n \sched_i=1\right \}$$
% we consider a discrete version of this family of functions, $DF$, where
% $\sched_i\in\{n^{-2}(1+\frac{1}{8\log n})^k,~0\leq k\leq  16\log^2 n\}$ and
% $\sum_{i=1}^n \sched_i \leq 1+\frac{1}{8\log n}.$ Thus, each $\sched_i$ is restricted to $16\log^2 n +1$ values, and rounding to this set of values increases the value of the functions we consider by no more than
%  $\left (\frac{\frac{1}{n^2} +(1+\frac{1}{8\log n})}{1-\frac{1}{8\log n}}\right )^2 \leq (1+\frac{1}{\log n})$.
%
% We apply Massart Lemma~\cite[Lemma 26.8]{shalev2014understanding} (adapted to our case):
% \begin{lemma}
% Let $DF=\{f_1,\dots, f_N\}$ be a set of $N$ functions, and let $x_1, \dots, x_m$ be $m$ samples.
% Let $$R=\max_{f\in F} \paran{\sum_{i=1}^m f(x_i)^2}^{1/2}$$ then
% $$R_m(F)\leq \frac{R\sqrt{2\log N}}{m}.$$
% \end{lemma}
% The number of functions in our case is $N=n(16\log n+1)^n$, and $R\leq \sqrt{m}$, since for any $\sched$ and $S$,
% $\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2 \leq 1$.
% Thus,
% $$R_m(F)\leq (1+\frac{1}{\log n}) R_m(DF)\leq (1+\frac{1}{\log n})\sqrt{\frac{{4n\log\log n}}{{m}}}.$$
%
% Applying Theorem~\ref{th:rc} we prove that when using a sample collection of size $m=4n\log n \log\log n$, with probability $1-\frac{1}{n}$, we approximate all the functions within an additive
%  factor of $\frac{1}{\log n}$.
%
%  Our analysis so far did not assume any particular propagation structure on the network. However, in novelty discovery in a social network we are mostly interested in discovering items that are distributed within small communities and can ignore items that are already distributed to a large fraction of the network.  Assume that the communities we are mostly interested in have size $\alpha(n)<<n$, then the number of functions in the corresponding set $DF$ is
%  bounded by $N=n(16\log n+1)^\alpha(n)$ and the corresponding sample size needed for the discovery is reduced by a multiplicative factor of $\alpha(n)/n$.
%
%
\subsection{Scaling up with MapReduce}\label{sec:mapreduce}
In this section, we discuss how to adapt \algoname and \algonameapx to the
MapReduce framework~\citep{dean2008mapreduce}. The discussion will focus on
adapting \algonameapx, as the extension to \algoname is immediate. We denote the
resulting algorithm as \algonamemr.

A MapReduce model consists of two parts: mapping by \texttt{mappers}, and
reducing by \texttt{reducers}. At each round of MapReduce the input dataset is
partitioned into independent chunks and each chunk is sent to a \texttt{mapper}.
Each mapper will process the received chunk and outputs pairs of key-values,
$\langle k, v \rangle$. Then, the  platform shuffles the key-value pairs and
aggregate the values of a same key and sends them to \texttt{reducers}.
Therefore, each \texttt{reducer} receives inputs of form $\langle k, (v_1,
\ldots, v_r) \rangle$, where $v_1, \ldots, v_r$ are all the values with the key
$k$ generated by \texttt{mappers}. Finally, \texttt{reducers} process their
inputs and return the final outputs.

In following, we show how computing the cost and $W_i$ functions (and
consequently $Q$)  can be done in a MapReduce given a sample $\Sample$ for a
generating process $\sys=(U,\family,\pi)$. Therefore, XXX can be easily scaled to larger dataset.
%for a generating process $\sys=(U,\family,\pi)$, with  full access to $\family$ and $\pi$. In a very similar argument, the method extends to the case when we only have access to a sample  $\Sample$ of $\family$ and we have to compute the cost and $W_i$ functions according to the sample $\Sample$.
So, lets assume the input dataset is $\Sample$, and the elements in $\Sample$ are partitioned and sent to \texttt{mappers}. We also assume that \texttt{mappers} have a copy of $\sched$.


\paragraph{\bf Computing the cost function.}
Each \texttt{mappers}, for every $S\in \Sample$ that it receives, outputs the following key-value pair:
$\left\langle 0,  \frac{1}{\ell(\Sample)}f(S) \right\rangle,$
where $f(S) = \frac{1}{1- \theta(1-p(S))^c}$.
Next, a \texttt{reducer} receives an input of the form
$\left\langle 0,  \left[\frac{1}{\ell(\Sample)}f(S)\right]_{S\in \Sample}  \right\rangle,$
and outputs $\frac{\sum_{S\in \Sample}f(S)}{\ell(\Sample)}$.

\paragraph{\bf Computing  the $W_i$'s functions.}
Each \texttt{mappers}, for every $S\in \Sample$ that it receives, outputs all the following key-value pairs:
$$\left\langle i,  \frac{-\theta c (1-\sched(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\sched(S))^c)^2} \right\rangle, \forall i \in S.$$
Next, each \texttt{reducer} receives an input of the form
$$\left\langle i,  \left[\frac{-\theta c (1-\sched(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\sched(S))^c)^2}\right]_{S\in \Sample: i\in S} \right\rangle,$$
and would be able to compute $\sched_iW_i(\sched, \Sample)$, by adding the ``values'' and multiply by $\sched_i$.
% $$\sched_iW_i(\sched) = \sum_{S: i\in S}\frac{-\theta}{\ell(\Sample)\paran{1-\theta(1-\sched(S))}^2}.$$
Finally, the aggregator (or the central processing unit) can normalize the vector $(\sched_1W_1(\sched), \ldots, \sched_nW_n(\sched))$, and compute $Q(\sched)$.

Using a very similar technique, one can compute the cost and $W_i$'s functions
in MapReduce framework when having full access to $\family$ and $\pi$ parameters
of the generating process $\sys=(U,\family,\pi)$.
%Each \texttt{mappers}, for every $S\in \Sample'$ that it receives, outputs all the following key-value pairs:
%$$\left\langle i,  \frac{\theta^2 f(S)^2}{|\Sample'|} \right\rangle, \forall i \in S.$$
%Next, each \texttt{reducer} receives an input of the form
%$$\left\langle i,  \left[\frac{\theta^2 f(S)^2}{|\Sample'|}\right]_{S\in \Sample': i\in S} \right\rangle,$$
%and would be able to compute the $i$-th coordinate of the gradient:
%$$\frac{\partial}{\partial \sched_i} \cost{\sched} = - \frac{\theta^2}{|\Sample'|}\sum_{S\in \Sample': i\in S} f(S)^2.$$

% Now, we show how each round of iteration in Algorithm~\ref{alg:optimizer} can be implemented in a MapReduce model: Note that the goal in each iteration is to compute
% $$c_i = \sum_{S\in \M: i\in S} \paran{\frac{\sched_i}{\sum_{j\in S}\sched_j}}^2,$$
% for $1 \leq i \leq n$. The input dataset is $\M$, and the informed sets in $\M$ are partitioned and sent to \texttt{mappers}. We also assume that \texttt{mappers} have a copy of $\sched$. Each \texttt{mapper} for any $S\in\M$ that it receives, outputs all the following key-value pairs:
% $$\langle i,  (\paran{\sched_i, p(S)})\rangle, \forall i \in S,$$
% where $p(S) = \sum_{j\in S} \sched_j$ (note that each value is also an ordered pair).
% Next, each \texttt{reducer} receives inputs of form $\langle i, [(\sched_i, p(S_1)), \ldots, (\sched_i, p(S_r))]\rangle$
% where $S_1, \ldots, S_r$ are all the informed set in $\M$ that include the node $i$, and outputs
% $$c_i = \sum_{t=1}^r \paran{\frac{\sched_i}{\sum_{j\in S_t} \sched_j}}^2,$$
% which is the result of one iteration in Algorithm~\ref{alg:optimizer}.
%
% Note that the running time of $\optimizer$ is $O(|\M|)$ (linear) given a constant number of iterations. In particular, if $|\M| = O(|V|+|E|)$ the whole process of sampling and optimizing is linear in the network size, where $V$ and $E$ are the nodes and edges sets of the network, respectively.
%
% Given a schedule $\sched=(\sched_1,\dots,\sched_n)$ the expected time to probe at least one node in a set $S$ is $\frac{1}{\sum_{j\in S}\sched_j} $. Since $\pi(S)$ is the probability of generating an item that is distributed to an informed set $S$, the cost of schedule $\sched$ is
% $\cost{\sched} =\sum_{S\subseteq V} \frac{\pi(S)}{\sum_{j\in S} \sched_j},$ and our goal is to compute an optimal schedule $\sched^*=\arg\min_{\sched} \cost{\sched,\pi}$. There are two main difficulties in computing the optimal schedule: (1) the number of terms in the expression for the cost is exponential in $n$; and (2) we do not know that distribution $\pi$, and it can vary in time.
%
% Our solution consists of two major components. We first prove, and demonstrate in simulations, that a relatively small sample size is sufficient to characterize
% a close to optimal schedule, we then show how to efficiently compute an optimal schedule with respect to the sample.
% Both parts use the following derivation for the cost function. Intuitively, this expression
% ``distribute'' the cost of finding an item between the vertices of the set that receives this item (we give more intuition for using this form later).
%
% \begin{eqnarray}
% \label{eq:cost}
% \cost{p}&= &\sum_{S\subseteq V} \frac{\pi(S)}{\sum_{j\in S} \sched_j} \nonumber \\
% &=& \sum_{i\in V} \sum_{S:i\in S} \frac{\pi(S)}{\sum_{j\in S} \sched_j} \frac{\sched_i}{\sum_{j\in S} \sched_j}  \nonumber
% \\ &=&
%  \sum_{i\in V} \frac{1} {\sched_i}\sum_{S:i\in S} \pi(S) \left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2
%  \end{eqnarray}
