\section{The \algonamebasecaps{} Algorithm}\label{sec:method}
In this section, we study the $(\theta,c)$-OPSP for a generating process
$\sys=(\family,\pi)$ on a graph $G=(V,E)$ and present the algorithm \algoname
(and its variants) to solve it.

We start we start by assuming that we have complete knowledge of $\sys$, i.e.,
we know $\family$ and $\pi$. This strong assumption allows us to study the
theoretical properties of the cost of a function and motivates the design of
our algorithm \algoname to compute an optimal schedule. We then remove the
assumption and show how we can extend \algoname to only use a collection of
observations from $\sys$. Then we discuss how to recalibrate our algorithms when
the parameters of the process (e.g., $\pi$ or $\family$) change over time.
Finally, we show an algorithm for the MapReduce framework that allows us to
scale to large networks.

\subsection{Computing the Optimal Schedule}\label{sec:optimize}
We now first conduct a theoretical analysis of the cost function $\cost_\theta$,
and then use the results to develop \algoname, our algorithm to compute the
optimal $c$-schedule (i.e., solve the $(\theta,c)$-OPSP).

\paragraph{Analysis of the cost function}
Assume for now that we know $\sys$, i.e., we have complete knowledge $\family$,
and $\pi$. With this assumption, we can exactly compute the $\theta$-cost of a
$c$-schedule.

\begin{lemma}\label{lem:explicit}
Let $\sched$ be a $c$-schedule. Then
\begin{equation}\label{eq:explicit}
	\cost(\sched) \defeq
	\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_\theta(t')] =
	\sum_{S\in \family} \frac{\pi(S)}{1- \theta(1-\sched(S))^c},
\end{equation}
where $\sched(S) = \sum_{v\in S} \sched_v$.
\end{lemma}

%\todo[All]{I reorganized the proof, so please review.}
\begin{proof}
	Let $t$ be a time step, and consider the quantity $\expect[L_\theta(t)]$. By
	definition we have
	\[
		\expect[L_\theta(t)]=\expect\left[\sum_{(t',S)\in
		N_t}\fresh_\theta(t,t',S)\right]=\expect\left[\sum_{(t',S)\in
		N_t}\theta^{t-t'}\right],
	\]
	where $N_t$ is the set of uncaught items at time $t$. Let now, for any
	$t'\le t$, $N_{t,t'}\subseteq N_t$ be the set of uncaught items in the form
	$(t',S)$. Then we can write
	\[
		\expect[L_\theta(t)]=\expect\left[\sum_{t'=0}^{t}\sum_{(t',S)\in
			N_{t,t'}}\theta^{t-t'}\right]\enspace.
	\]
	Define now, for each $S\in\family$, the random variable $X_{S,t',t}$ which
	takes the value $\theta^{t-t'}$ if $(t',S)\in N_{t,t'}$, and $0$ otherwise.
	Using the linearity of expectation, we can write:
	\begin{align}\label{eq:loadexp}
		\expect[L_\theta(t)]&=\sum_{S\in\family}\sum_{t'=0}^t\expect[X_S,t',t]\nonumber\\
		&=\sum_{S\in\family}\sum_{t'=0}^t
		\theta^{t-t'}\Pr(X_{S,t,t'}=\theta^{t-t'})\enspace.
	\end{align}

	The r.v. $X_{S,t,t'}$ takes value $\theta^{t-t'}$ if and only if the
	following two events $E_1$ and $E_2$ both take place:
	\begin{itemize*}
		\item $E_1$: the set $S\in F$ belongs to $\Sample_{t'}$, i.e., is
			generated by $\sys$ at time $t'$;
		\item $E_2$: the item $(t',S)$ is uncaught at time $t$. This is
			equivalent to say that no node $v\in S$ was probed in the time
			interval $[t',t]$.
	\end{itemize*}
	We have $\Pr(E_1)=\pi(S)$, and
	\[
		\Pr(E_2)=(1-\sched(S))^{c(t-t')}\enspace.
	\]
	The events $E_1$ and $E_2$ are independent, as the event of process of
	probing the nodes is independent from the process of generating items,
	therefore, we have
	\[
		\Pr(X_{S,t,t'}=\theta^{t-t'})=\Pr(E_1)\Pr(E_2)=\pi(S)(1-\sched(S))^{c(t-t')}\enspace.
	\]
	We can plug this quantity in the rightmost term of~\eqref{eq:loadexp} and
	write
	\begin{align}
		\lim_{t\rightarrow\infty}\expect[L_\theta(t)]
		&=\lim_{t\rightarrow\infty}\sum_{S\in\family}\sum_{t'=0}^t\theta^{t-t'}\pi(S)(1-\sched(S))^{c(t-t')}\nonumber\\
		&=\lim_{t\rightarrow\infty}\sum_{S\in\family}\pi(S)\sum_{t'=0}^t(\theta(1-\sched(S))^c)^{t}\nonumber\\
		&=\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c},
	\end{align}
	where we used the fact that $\theta(1-\sched(S))^c< 1$. We just showed that
	the sequence $(\expect[L_\theta(t)])_{t\in\mathbb{N}}$ converges as $t\rightarrow \infty$. Therefore,
	its Ces\`aro mean, i.e.,
	$\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_\theta(t)]$,
	equals to its limit~\citep[Sect.~5.4]{Hardy91} and we have
	\begin{align*}
		\cost_\theta(\sched)&=\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_\theta(t)] =
		\lim_{t\rightarrow\infty} \expect[L_\theta(t)]\\
%		&=
%		\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t \sum_{S\in\family}\pi(S)\sum_{t'=0}^t(\theta(1-\sched(S))^c)^{t'}\\
%		&=\lim_{t\rightarrow\infty}\frac{1}{t}t\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c}\\
		&=
		\sum_{S\in\family}\frac{\pi(S)}{1-\theta(1-\sched(S))^c}\enspace.
	\end{align*}
\end{proof}

We now show that $\cost_\theta(\sched)$, as expressed by the
r.h.s.~of~\eqref{eq:explicit} is a convex function over its domain
$\mathsf{S}_c$, the set of all possible $c$-schedules. We then use this result
to show how to compute an optimal schedule.

\begin{theorem}\label{thm:convexity}
	The cost function $\cost_\theta(\sched)$ is a convex function over
	$\mathsf{S}_c$.
\end{theorem}
\begin{proof}
	For any $S\in\family$, let
	\[
		f_S(\sched) = \frac{1}{1 - \theta(1-\sched(S))^c}\enspace.
	\]
	The function $\cost_\theta(\sched)$ is a linear combination of
	$f_S(\sched)$'s with positive coefficients. Hence to show that
	$\cost_\theta(\sched)$ is convex it is sufficient to show that, for any
	$S\in\family$, $f_S(\sched)$ is convex.

	We start by showing that $g_S(\sched) = \theta(1-\sched(S))^c$ is convex.
	This is because its Hessian matrix is positive semidefinite (see~\cite{boyd2004convex}). Indeed we have:
	\[
		\frac{\partial}{\partial \sched_i \partial \sched_j} g_S(\sched)=\left\{
		\begin{array}{lr}
		\theta c(c-1) (1-\sched(S))^{c-2} &  i,j \in S\\
		0 &  \text{otherwise}
		\end{array}
		\right.
	\]
	Let $\mathbf{v}_S$ be a $n\times 1$ vector in $\mathbb{R}^n$ such that its
	$i$-th coordinate is $\left[c(c-1) (1-\sched(S))^{c-2} \right]^{1/2}$ if $i\in
	S$, and $0$ otherwise. We can write the Hessian matrix of $g_S$ as
	\[
		\nabla^2 g_S = V_S * V_S^\mathsf{T},
	\]
	and thus, $\nabla^2 g_S$ is positive semidefinite matrix and $g$ is convex.
	From here, we have that $1-g_S$ is a \emph{concave} function. Since
	$f_S(\sched)=\frac{1}{1-g_S(\sched)}$ and the function $h(x)=\frac{1}{x}$ is
	convex and non-increasing, then $f_S$ is a convex function.
\end{proof}

If for every $v\in V$, $S=\{v\}$ belongs to $\family$, then the
function $g_S$ in the above proof is \emph{strictly} convex, and so is $f_S$.

We then have the following corollary of Thm.~\ref{thm:convexity}.

\begin{corollary}\label{corol:convexity}
	Any schedule $\sched$ with locally minimum cost is an optimal schedule
	(i.e., it has global minimum cost). Furthermore, if for every $v\in V$,
	$\{v\}$ belongs to $\family$, the optimal schedule is \emph{unique}.
\end{corollary}

\paragraph{The algorithm}
Corollary~\ref{corol:convexity} implies that one can compute an optimal
$c-$schedule $p^*$ (i.e., solve the $(\theta,c)$-OPSP) by solving the
unconstrained minimization of $\cost_\theta$ over the set $\mathsf{S}_c$ of all
$c$-schedules, or equivalently by solving the following constrained minimization
problem on $\mathbb{R}^n$:
\begin{equation}\label{eq:optimization}
	\boxed{
	\begin{array}{rrclcl}
	\displaystyle \min_{\sched\in\mathbb{R}^n} & \multicolumn{3}{l}{\cost_\theta(\sched)} \\
	&\displaystyle \sum_{i=1}^{n} \sched_i & = & 1 \\
	&\sched_i & \geq & 0 & & \forall i \in \{1,\ldots,n\} \\
	\end{array} }
\end{equation}
Since the function $\cost_\theta$ is convex and the constraints are linear, the
optimal solution can, theoretically, be found efficiently~\citep{BoydV04}. In
practice though, available convex optimization problem solvers can not scale
well with the number $n$ of variables, especially when $n$ is in the millions
as is the case for modern graphs like online social networks or the Web. Hence
we developed \algoname, an iterative method based on \emph{Lagrange
multipliers}\citep[Sect.~5.1]{BodydV04}, which can scale efficiently and can be
adapted to the MapReduce framework of computation~\citep{dean2008mapreduce}, as
we show in Sect.~\ref{sec:mapreduce}. While we can not prove that this iterative
method always converges, we can prove (Thm.~\ref{thm:optimal}) that (i) if at
any iteration the algorithm examines an optimal schedule, then it will reach
convergence at the next iteration, and (ii) if it converges to a schedule, that
schedule is optimal. In Sect.~\ref{sec:exp} we show our experimental results
illustrating the convergence of \algoname in different cases.

\algoname takes as inputs the collection $\family$, the function $\pi$, and the
parameters $c$ and $\theta$, and outputs a schedule $\sched$ which, if
convergence (defined in the following) has been reached, is the optimal
schedule. It starts from a uniform schedule $\sched^{(0)}$, i.e.,
$\sched^{(0)}_i=1/n$ for all $1\le i\le n$, and iteratively refines it until
convergence (or until a user-specified maximum number of iterations have been
performed). At iteration $j\ge 1$, we compute, for each value $i$, $1\le i\le
n$, the function
\begin{equation}\label{eq:functionw}
	W_i(\sched^{(j-1)}) \defeq \sum_{\substack{S \in \family \\
		\mbox{\scriptsize{s.t. }} i\in
S}} \frac{\theta c \pi(S)
	(1-\sched^{(j-1)}(S))^{c-1}}{(1-\theta(1-\sched^{(j-1)}(S))^c)^2}
\end{equation}
and then set
\[
	\sched^{(j)}_i = \frac{\sched^{(j-1)}_i W_i(\sched^{(j-1)})}{\sum_{z=1}^n
	\sched^{(j-1)}_z W_z(\sched^{(j-1)})}\enspace.
\]
The algorithm then checks whether $\sched^{(j)}=\sched^{(j-1)}$. If so, then we
reached convergence and we can return $\sched^{(j)}$ in output, otherwise we
perform iteration $j+1$. The pseudocode for \algoname is in
Algorithm~\ref{alg:iterative}. The following theorem shows the correctness of
the algorithm in case of convergence.

\begin{theorem}\label{thm:optimal}
	We have that:
	\begin{enumerate*}
		\item if at any iteration $j$ the schedule $\sched^{(j)}$ is optimal,
			then \algoname reaches convergence at iteration $j+1$; and
		\item if \algoname reaches convergence, then the returned schedule
			$\sched$ is optimal.
	\end{enumerate*}
\end{theorem}

\begin{proof}
	From the method of the Lagrange multipliers~\citep[Sect.~5.1]{BoydV04}, we
	have that, if a schedule $\sched$ is optimal, then there exists a value
	$\lambda\in\mathbb{R}$ such that $\sched$ and $\lambda$ form a solution to
	the following system of $n+1$ equations in $n+1$ unknowns:
	\begin{equation}\label{eq:lagrange}
		\nabla [\cost_\theta(\sched) + \lambda (\sched_1+\ldots+\sched_n-1)] = 0,
	\end{equation}
	where the gradient on the l.h.s.~is taken w.r.t.~(the components of)
	$\sched$ and to $\lambda$ (i.e., has $n+1$ components).

	For $1\le i\le n$, the $i$-th equation induced by~\eqref{eq:lagrange} is
	\[
		\frac{\partial}{\partial \sched_i} \cost_\theta(\sched) + \lambda = 0,
	\]
	or, equivalently,
	\begin{equation}\label{eq:lagrange_i}
		\sum_{\substack{S\in\family\\\mbox{s.t.} i\in S}} \frac{\theta c
			\pi(S) (1-\sched(S))^{c-1}}{(1-\theta(1-\sched(S))^c)^2} =
			\lambda\enspace.
	\end{equation}
	The term on the l.h.s.~is exactly $W_i(\sched)$.
	The $n+1$-th equation of the system~\eqref{eq:lagrange} (i.e., the one
	involving the partial derivative w.r.t.~$\lambda$) is
	\begin{equation}\label{eq:lagrange_l}
		\sum_{z=1}^n \sched_z = 1\enspace.
	\end{equation}

	Consider now the point 1 in the statement of the theorem, and assume that we
	are at iteration $j$ such that $j$ is the minimum iteration index for which
	the schedule $\sched^{(j)}$ computed at the end of iteration $j$ is optimal.
	Then, for any $i$, $1\le i\le n$, we have
	\[
		W_i(\sched^{(j)})=\lambda
	\]
	because $\sched^{(j)}$ is optimal and hence all identities in the form
	of~\eqref{eq:lagrange_i} must be true. For the same reason,
	\eqref{eq:lagrange_l} must also hold for $\sched^{(j)}$, so we have
	\[
		\sum_{z=1}^n \sched^{(j)}_z = 1\enspace.
	\]
	Hence, for any $1\le i\le n$, we can write the value $\sched^{(j+1)}_i$
	computed at the end of iteration $j+1$ as
	\[
		\sched^{(j+1)}_i = \frac{\sched^{(j)}_i W_i(\sched^{(j)})}{\sum_{z=1}^n
		\sched^{(j)}_z W_z(\sched^{(j)})}=
		\frac{\sched^{(j)}_i\lambda}{1\lambda}=\sched^{(j)}_i,
	\]
	which means that we reached convergence and \algoname will return
	$\sched^{(j+1)}$ in output, which is optimal because
	$\sched^{(j+1)}=\sched^{(j)}$.

	Consider now point 2 in the statement of the theorem, and let $j$ be the
	first iteration for which $\sched^{(j)}=\sched^{(j-1)}$. Then we have, for
	any $1\le i\le n$,
	\[
		\sched^{(j)}_i = \frac{\sched^{(j-1)}_i W_i(\sched^{(j-1)})}{\sum_{z=1}^n
		\sched^{(j-1)}_z W_z(\sched^{(j-1)})} = \sched^{(j-1)}_i\enspace.
	\]
	This implies
	\begin{equation}\label{eq:identityw}
		W_i(\sched^{(j-1)}) = \sum_{z=1}^n \sched^{(j-1)}_z W_z(\sched^{(j-1)})
	\end{equation}
	and the r.h.s.~does not depend on $i$, and so neither does
	$W_i(\sched^{(j-1)})$. Hence we have
	$W_1(\sched^{(j-1)})=\dotsb=W_n(\sched^{(j-1)})$ and can
	rewrite~\eqref{eq:identityw} as
	\[
		W_i(\sched^{(j-1)}) =  \sum_{z=1}^n \sched^{(j-1)}_z
		W_i(\sched^{(j-1)}),
	\]
	which implies that the identity~\eqref{eq:lagrange_l} holds for
	$\sched^{(j-1)}$. Moreover, if we set
	\[
		\lambda = W_1(\sched^{(j-1)})
	\]
	we have that all the identities in the form of~\eqref{eq:lagrange_i} hold.
	Then, $\sched^{(j-1)}$ and $\lambda$ form a solution to the
	system~\eqref{eq:lagrange}, which implies that $\sched^{(j-1)}$ is optimal
	and so must be $\sched^{(j)}$, the returned schedule, as it is equal to
	$\sched^{(j-1)}$ because \algoname reached convergence.
\end{proof}

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKwComment{tcp}{//}{}
	\SetKw{KwBreak}{break}
	\Input{$\family$, $\pi$, $c$, $\theta$, and maximum number $T$ of iterations}
	\Output{An optimal $c$-schedule $\sched$ with minimum $\theta$-cost}
	\For{$i\leftarrow 1$ \KwTo $n$}{
		$\sched_i \leftarrow 1/n$\;
	}
	\For{$j\leftarrow 1$ \KwTo $T$} {
		\For{$i\leftarrow 1$ \KwTo $n$}{
			$W_i\leftarrow 0$\;
		}
		\For{$S\in \family$} {\label{algline:sum}
			\For{$i\in S$} {
				$W_i \leftarrow W_i + \theta c \pi(S) (1-\sched(S))^{c-1}/(1-\theta(1-\sched(S))^c)^2$\label{algline:w}\;
			}
		}
		$\sched_{\mathrm{old}}\leftarrow \sched$\;
		\For{$i\leftarrow 1$ \KwTo $n$}{
			$\sched_i\leftarrow \frac{\sched_iW_i}{\sum_{i} \sched_iW_i}$\;
		}
		\If(// test for convergence){$\sched_{\mathrm{old}} = \sched$} {
			%\tcp{Reached convergence}
			\KwBreak\;
		}
	}
	\Return{$\sched$}\;
	\caption{\algoname}
	\label{alg:iterative}
\end{algorithm}
% Matteo 7/1. The following sentence is commented because we do not know whether
% is true. It is not clear what $F$ is to me. Ahmad may have an idea.
%``However, this iterative method is convergent if the Jacobian norm of $F$ is
%less than 1, i.e., $\|DF\| < 1$.''

\subsection{Approximation through sampling}\label{sec:sampcomp}
We now remove the assumption, not realistic in practice, of knowing the
generating process $\sys$ exactly through $\family$ and $\pi$. Instead, we
observe the process using, for a certain time, a schedule that iterates over
all nodes (or a schedule that selects each node with uniform probability), until
we have observed, for each time step $t$ in a limited time interval $[a,b]$, the
set $\Sample_t$ generated by $\sys$, and therefore we have access to a
collection
\begin{equation}\label{eq:sample}
	\Sample=\{\Sample_a,\Sample_{a+1},\dotsc,\Sample_b\}
\end{equation}
We usually refer to $\Sample$ as a \emph{sample gathered in the time interval
$[a,b]$}. Here we show that the cost of a schedule can be approximated within a
multiplicative factor $\varepsilon\in[0,1]$ if $b-a=O(\varepsilon^2\log n)$, and
that it is possible to adapt \algoname to this case.

We start by defining the cost of a schedule w.r.t.~to a sample $\Sample$.

\begin{definition}\label{def:costsample}
	Suppose $\sched$ is a $c$-schedule and $\Sample$ is as in~\eqref{eq:sample},
	with $b-a=\ell$. The $\theta$-cost of $\sched$ w.r.t.~to $\Sample$
	denoted by $\cost_\theta(\sched,\Sample)$ is defined as
	\[
		\cost_\theta(\sched,\Sample)\defeq\frac{1}{\ell}\sum_{S\in \Sample}
		\frac{1}{1-\theta(1-\sched(S))^c}\enspace.
	\]
\end{definition}

For $1\le i\le n$, define now the functions
\[
	W_i(\sched,\Sample) = \frac{1}{\ell(\Sample)}\sum_{S\in \Sample: i\in S}
	\frac{\theta c (1-\sched(S))^{c-1}}{(1-\theta(1-\sched(S))^c)^2}\enspace.
\]

We can then define a variant of \algoname, which we call \algonameapx. The
differences from \algoname are:
\begin{enumerate*}
	\item the loop on line~\ref{algline:sum} in Alg.~\ref{alg:iterative} is only
		over the sets that appear in at least one $\Sample_j\in\Sample$.
	\item \algonameapx uses the values $W_i(\sched,\Sample)$ instead of
		$W_i(\sched)$ (line~\ref{algline:w} in Alg.~\ref{alg:iterative});
\end{enumerate*}

If \algonameapx reaches convergence, it returns a schedule with the minimum cost
w.r.t.~the sample $\Sample$. More formally, by following the same steps as in
the proof of Thm.~\ref{thm:optimal}, we can prove the following result about
\algonameapx.

\begin{lemma}\label{lem:optimal_sample}
	We have that:
	\begin{enumerate*}
		\item if at any iteration $j$ the schedule $\sched^{(j)}$ has minimum
			cost w.r.t.~$\Sample$, then \algonameapx reaches convergence at
			iteration $j+1$; and
		\item if \algonameapx reaches convergence, then the returned schedule
			$\sched$ has minimum cost w.r.t.~$\Sample$.
	\end{enumerate*}
\end{lemma}

Let $\ell(\Sample)$ denote the length of the time interval during which
$\Sample$ was collected. For a $c$-schedule $\sched$,
$\cost_\theta(\sched,\Sample)$ is an approximation of $\cost_\theta(\sched)$,
and intuitively the larger $\ell$, the better the approximation.

We now show that, if $\ell(\Sample)$ is large enough, then, with high
probability (i.e., with probability at least $1-1/n^r$ for some constant $r$),
the schedule $\sched$ returned by \algonameapx in case of convergence has a cost
$\cost_\theta(\sched)$ that is close to the cost $\cost_\theta(\sched^*)$ of an optimal
schedule $\sched^*$.  Specifically, we show the following.

\begin{theorem}\label{thm:approx_sample}
	Let $r$ be a positive integer, and let $\Sample$ be a sample gathered during a
	time interval of length
	\begin{equation}\label{eq:samplesize}
		\ell(\Sample) \geq
		\frac{3(r\ln(n)+\ln(4))}{\varepsilon^2(1-\theta)}\enspace.
	\end{equation}
	Let $\sched^*$ be an optimal schedule, i.e., a schedule with minimum cost. If \algonameapx converges, then the returned schedule
	$\sched$ is such that
	\[
		\cost_\theta(\sched^*)\le\cost_\theta(\sched)\le\frac{1+\varepsilon}{1-\varepsilon}\cost_\theta(\sched^*)\enspace.
	\]
\end{theorem}

Before we can prove Thm.~\ref{thm:approx_sample}, we need to show the following
technical lemma.

\begin{lemma}\label{lem:chernoffcost}
	Let $\sched$ be a $c$-schedule and $\Sample$ be a sample gathered during a
	time interval of length
	\[
		\ell(\Sample) \geq \frac{3(r\ln(n)+\ln(2))}{\varepsilon^2(1-\theta)},
	\]
	where $r$ is any natural number. Then, for every schedule $\sched$ we have
	\[
		\Pr(|\cost_\theta(\sched,\Sample) - \cost_\theta(\sched)|\geq
		\varepsilon\cdot\cost_\theta(\sched)) < \frac{1}{n^r}\enspace .
	\]
\end{lemma}

\begin{proof}
	For any $S\in\family$, let $X_S$ be a random variable which is
	$\frac{1}{1-\theta(1-\sched(S))^c}$ with probability $\pi(S)$, and zero
	otherwise. Since $\sched(S)\in [0,1]$, we have
	\[
		1\leq X_S \leq \frac{1}{1-\theta}\enspace.
	\]
	If we let $X = \sum_{S\in \family} X_S$, then
	\begin{equation}\label{eq:boundcost}
		\cost_\theta(\sched) = \expect[X] = \sum_{S\in\family} \expect[X_{S}]
		\geq \sum_{S\in\family}\pi(S)\enspace.
	\end{equation}
	Let $Z=\sum_{S\in\family}\pi(S)$. Then we have
	\[
		Z \leq X \leq \frac{Z}{1-\theta}\enspace.
	\]
	Let $X^i_S$ be the $i$-th draw of $X_S$ during the time interval $\Sample$
	was sampled from and define $X^i = \sum_{S\in\family}X_S^i$. We have
	\[
		\cost_\theta(\sched,\Sample) =
		\frac{1}{\ell(\Sample)}\sum_{i}X^i\enspace.
	\]
	Let now
	\[
		\mu=\frac{\ell(\Sample)(1-\theta)}{|Z|}\cost_\theta(\sched)\enspace.
	\]
	By using the Chernoff bound for Binomial random
	variables~\citep[Corol.~4.6]{MitzenmacherU05}, we have
	\begin{align*}
		&\Pr\left(\left|\cost_\theta(\sched,\Sample) - \cost_\theta(\sched)\right| \geq \varepsilon
		\cost_\theta(\sched)\right)\\
		&=\Pr\left(\left|\sum_i X^i - \ell(\Sample)\cost_\theta(\sched)\right| \geq \varepsilon
		\ell(\Sample) \cost_\theta(\sched)\right) \\
		&= \Pr\left(\left|\frac{1-\theta}{|Z|}\sum_i X^i - \mu\right| \geq \varepsilon
		\mu\right) \leq 2\exp\left(-\frac{\varepsilon^2\mu}{3}\right)\\
		&\le 2\exp\left(-\frac{\varepsilon^2 \ell(\Sample) (1-\theta)
			\cost_\theta(\sched)}{3|Z|}\right) \leq 2\exp\left(-\frac{\varepsilon^2 \ell(\Sample) (1-\theta)}{3}\right),
	\end{align*}
	where the last inequality follows from the rightmost inequality
	in~\eqref{eq:boundcost}. The thesis follows from our choice of
	$\ell(\Sample)$.
\end{proof}

We can now prove Thm.~\ref{thm:approx_sample}.
\begin{proof}[of Thm.~\ref{thm:approx_sample}]
	The leftmost inequality is immediate, so we focus on the one on the right.
	For our choice of $\ell(\Sample)$ we have, through the union bound, that,
	with probability at least $1-1/n^r$, at the same time:
	\begin{align}\label{eq:boundedcosts}
		(1-\varepsilon)\cost_\theta(\sched)&\leq \cost_\theta(\sched,\Sample) &\le
		(1+\varepsilon)\cost_\theta(\sched) & \mbox{, and}\nonumber\\
		(1-\varepsilon)\cost_\theta(\sched^*)&\leq \cost_\theta(\sched^*,\Sample) &\le
		(1+\varepsilon)\cost_\theta(\sched^*)&
	\end{align}
	Since we assumed that \algonameapx reached convergence when computing
	$\sched$, then Thm.~\ref{thm:approx_sample} holds, and $\sched$ is a
	schedule with minimum cost w.r.t.~$\Sample$. In particular, it must be
	\[
		\cost_\theta(\sched,\Sample)\le \cost_\theta(\sched^*,\Sample)\enspace.
	\]
	From this and~\eqref{eq:boundedcosts}, We then have
	\[
		(1-\varepsilon)\cost_\theta(\sched)\leq \cost_\theta(\sched,\Sample) \le
		\cost_\theta(\sched^*,\Sample)\le (1+\varepsilon)\cost_\theta(\sched^*)
	\]
	and by comparing the leftmost and the rightmost terms we get the thesis.
\end{proof}

% Matteo commented out on 7/1
%Using a very similar argument we have the following theorem:
%\begin{theorem}\label{thm:chernoffW}
% Suppose $\Sample$ is a sample gathered during a time interval of length
% $\ell(\Sample) \geq \frac{3(r\log(n)+\log(2))}{\varepsilon^2(1-\theta)} =
% O\left(\frac{\log(n)}{\varepsilon^2}\right)$. Then, for every schedule $\sched$ and
% $i\in\{1,\dots,n\}$ we have
% $$\Pr(|W_i(\sched,\Sample) - W_i(\sched)|\geq \varepsilon\cdot W_i(\sched)) < \frac{1}{n^r}.$$
%\end{theorem}


\subsection{Learning Dynamic Parameters}\label{sec:dynamic}
\todo[Matteo]{I haven't done this yet. Ahmad and I discussed a bit about it, and
I am still not sure about how to present some limitations carefully\ldots}
In this short section, we consider the case when the parameters of the network change (i.e, the distribution over the possible informed-sets changes).  To apply our method to a dynamic environment we first sample the process to generate a sufficiently large sample collection. This can be done by probing
 all nodes with uniform distribution during a short time interval, or using a round robin schedule (Algorithm~\ref{alg:sampler}). For an item the sampling process stores the set of nodes that received this item. We then compute an optimal schedule with respect to that sample (Algorithm~\ref{alg:iterative}). We use this schedule and monitor the cost to detect significant changes. In that case we obtain a new sample, optimize with respect to that sample and apply the new schedule.

 Note that when we adapt our schedule to the new environment (using the most recent sample) the system converges to its stable setting exponentially (in $\theta$) fast: Suppose $L$ items have been generated since the change of the parameters until we adapt the new schedule. These items, if not caught, loose their freshness exponentially fast: after $t$ steps their freshness is at most $L\theta^t$ and gets diminished very quickly.

 In our experiments we provide different examples that illustrate how load of the generating process stables after the algorithm adapts itself to the changes of parameters (see Section\ref{sec:exp}).


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[!h]
\BlankLine
{\bf Inputs:} The length of the time interval that we sample from, $\texttt{time\_length}$.

\Begin{
$\D_1 \leftarrow \emptyset$\;
$\D_2 \leftarrow \emptyset$\;
 \For{nodes $u\in U$}{
   \For{items $i$ at $u$, not older than \texttt{time\_length} time steps}{
	   $\D_1 \leftarrow \D_1 \cup \{i\}$\;
      $\D_2 \leftarrow \D_2 \cup \{(i,u)\}$\;
      }
    }


$\Sample \leftarrow \emptyset$\;
\For{$i \in D_1$}{
  $S \leftarrow \emptyset$\;
  \For{$(i, u) \in D_2$}{
  $S \leftarrow S \cup \{u\}$\;
  }
  $\Sample \leftarrow \Sample \cup \{S\}$\;
  }

  return $\Sample$\;

}

\caption{XXXX}\label{alg:sampler}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}[!h]
% \BlankLine
% {\bf Inputs:} A sample collection, $\M$, and the number of iterations, $\texttt{iter}$.
%
% \Begin{
% $\sched\leftarrow (1/n,1/n,\ldots,1/n)$\;
%  \For{$t\in\set{1,\ldots,\texttt{iter}}$}{
%   $c\leftarrow (0,\ldots,0)$\;
%    \For{$S \in \M$}{
%      \For{$i \in \Sample$}{
%       $c_i \leftarrow c_i + \paran{\frac{\sched_i}{\sum_{j\in S \sched_j}}}^2$ \;
%       }
%     }
%
%   }
% \For{$i\in \set{1,\ldots,n}$}{
%   $\sched_i \leftarrow \frac{1}{\sum_{i=1}^n c_i}(c_1, \ldots, c_n)$\;
%   }
% return $\sched$\;
% }
%
% \caption{$\optimizer(\M, \texttt{iter})$}\label{alg:optimizer}
% \end{algorithm}









%  Outline of the algorithm is given in Figure~\ref{??}.

% \subsection{Sufficient sample}
% Our goal is to compute an (approximately) optimal schedule using one sample collection
% ${\cal M}=\{S_1,\dots,S_m\}$, of $m$ informed sets generated according to the distribution $\pi$.
% Identifying an optimal function among a set of functions using one sample collection requires strong statistical properties of the sample.
% We need to show that the cost of {\em any} schedule $\sched$, considered though out the optimization process, on the sample is a close approximation of the cost of $\sched$ on the entire distribution $\pi$.
% In particular we require that for very $i=1,\dots,n$, and any schedule $\sched$, the estimate
% $${C}_i(\sched,\M)=\frac{1}{m}\sum_{i=1}^m \left( \frac{\sched_i}{\sum_{j\in S_i} \sched_j} \right )^2$$ is sufficiently good estimate for
% $${C}_i(\sched)=\sum_{S:i\in S} \pi(S) \left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2.$$
% Thus, we need a {\em uniform convergence result} of the sample with respect to this functions over all possible schedules.
%
% We use the {\em Rademacher complexity} method~\cite{shalev2014understanding} to analyze our sample:
% Given a set of functions $F$ and a set of $m$ samples $M=z_1,\dots,z_m$, the
% %Rademacher complexity $R_m(F)$ and the
% empirical Rademacher sum $R_M(F)$
% %are
% %is $$R_m(F) = E_S [E_{\bar{\sigma}} [\sup_{f \in F}  ( \frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i )]], $$
% is $$ R_M (F) = E_{\bar{\sigma}} \left[\sup_{f \in F} \paran{\frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i} \right],$$
% %respectively,
% where $\bar{\sigma}=(\sigma_1,\dots, \sigma_m)$ is a vector of $m$ independent random variables, with $Pr(\sigma_i =1)=Pr(\sigma_i = -1)=1/2$.
% %The role of the $\sigma$ variables is to create a kind of output noise in the dataset. The supremum carries out an optimization process over all functions in %$F$ trying to fit this noise. The Rademacher complexity is high if the function class is successful at fitting this noise and low otherwise.
% Like the VC-dimension (for range spaces), the Rademacher sum is a measure of the expressiveness or complexity of a set of functions $F$. We use the following relation between the Rademacher sum and the uniform convergence of a sample:
%
% \begin{theorem}~\cite[Theorem 3.1]{mohri2012foundations}
% \label{th:rc}
% Let $F$ be a (finite or measurable) set of functions on a domain $Z$ such that for all $f\in F$, $f:Z\rightarrow [0,1]$. Let  $M=\{z_1,\dots, z_m\}$ be a sample chosen from a distribution $D$ on $Z$.
% With probability $1-\delta$ over the choices of $M$,
% %over the distribution of $S$,
% simultaneously for all functions $f\in F$, we have
% %$$E_D[f(z)] \le E_S[f(z)] + 2 R_m(F) + \sqrt{\frac{\ln(1/\delta)}{2m}},$$
% %and
% %$$ E_D[f(z)] \le E_S[f(z)] + 2 R_S(F) + 3\sqrt{\frac{\ln(2/\delta)}{2m}}.$$
% \begin{eqnarray}
% \label{bound1}
% E_D[f(z)] \le \hat{E}_M [f(z)] + 2 R_M (F) + 3\sqrt{\frac{\ln(1/\delta)}{2m}},
% \end{eqnarray}
% \end{theorem}
%
% To apply this theorem to our problem we observe that
% $$\frac{{C}_i(\sched)}{ \sum_{S:i\in S} \pi(S) }=\frac{1}{ \sum_{S:i\in S} \pi(S) }\sum_{S:i\in S} \pi(S) \left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2$$
% is the expectation of the function $f^i_{\sched}(S)=\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2$ over the distribution $D_{|i}$ (the distribution of sets that include node $i$). To bound the Rademacher empirical sum of the set of functions
% $$F=\left \{f^i_{\sched}(S)=\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2~\big |~1\leq i\leq n,~\sched\in [0,1]^n, ~\sum_{i=1}^n \sched_i=1\right \}$$
% we consider a discrete version of this family of functions, $DF$, where
% $\sched_i\in\{n^{-2}(1+\frac{1}{8\log n})^k,~0\leq k\leq  16\log^2 n\}$ and
% $\sum_{i=1}^n \sched_i \leq 1+\frac{1}{8\log n}.$ Thus, each $\sched_i$ is restricted to $16\log^2 n +1$ values, and rounding to this set of values increases the value of the functions we consider by no more than
%  $\left (\frac{\frac{1}{n^2} +(1+\frac{1}{8\log n})}{1-\frac{1}{8\log n}}\right )^2 \leq (1+\frac{1}{\log n})$.
%
% We apply Massart Lemma~\cite[Lemma 26.8]{shalev2014understanding} (adapted to our case):
% \begin{lemma}
% Let $DF=\{f_1,\dots, f_N\}$ be a set of $N$ functions, and let $x_1, \dots, x_m$ be $m$ samples.
% Let $$R=\max_{f\in F} \paran{\sum_{i=1}^m f(x_i)^2}^{1/2}$$ then
% $$R_m(F)\leq \frac{R\sqrt{2\log N}}{m}.$$
% \end{lemma}
% The number of functions in our case is $N=n(16\log n+1)^n$, and $R\leq \sqrt{m}$, since for any $\sched$ and $S$,
% $\left( \frac{\sched_i}{\sum_{j\in S} \sched_j} \right )^2 \leq 1$.
% Thus,
% $$R_m(F)\leq (1+\frac{1}{\log n}) R_m(DF)\leq (1+\frac{1}{\log n})\sqrt{\frac{{4n\log\log n}}{{m}}}.$$
%
% Applying Theorem~\ref{th:rc} we prove that when using a sample collection of size $m=4n\log n \log\log n$, with probability $1-\frac{1}{n}$, we approximate all the functions within an additive
%  factor of $\frac{1}{\log n}$.
%
%  Our analysis so far did not assume any particular propagation structure on the network. However, in novelty discovery in a social network we are mostly interested in discovering items that are distributed within small communities and can ignore items that are already distributed to a large fraction of the network.  Assume that the communities we are mostly interested in have size $\alpha(n)<<n$, then the number of functions in the corresponding set $DF$ is
%  bounded by $N=n(16\log n+1)^\alpha(n)$ and the corresponding sample size needed for the discovery is reduced by a multiplicative factor of $\alpha(n)/n$.
%
%
\subsection{Scaling up with MapReduce}\label{sec:mapreduce}
In this section, we discuss how to adapt \algoname and \algonameapx to the
MapReduce framework~\citep{dean2008mapreduce}. The discussion will focus on
adapting \algonameapx, as the extension to \algoname is immediate. We denote the
resulting algorithm as \algonamemr.

In the MapReduce framework, algorithms work in \emph{rounds}. At each round,
first a function $\mathsf{map}$ is executed independently (and therefore
potentially massively in parallel) on each element of the input, and a number
(or zero) key-value pairs of the form $(k,v)$ are emitted.  Then, in the second
part of the round, the emitted pairs are partitioned by keys and elements with
the same key are sent to the same machine (called the \emph{reducer} for that
key), where a function $\mathsf{reduce}$ is applied to the whole set of received
pairs, to emit the final output.

\todo[Ahmad]{Please review: \ahmad{2 rounds is enough, the aggregator that receives $(p_1W_1,\ldots,p_nW_n)$ can normalize it itself.}}

Each iteration of \algonameapx is spread over three rounds of \algonamemr. At
each round, we assume that the current schedule $\sched$ is available to all
machines (this is done in practice through a distributed cache). In the first
round, we compute the values $p_iW_i$, $1\le i\le n$, in the second round these
values are summed to get the normalization factor, and in the third round the
schedule $\sched$ is updated. The input in the first round are the sets
$S\in\Sample$. The function $\mathsf{map}_1(S)$ outputs, a key-value pair
$(i,v_S)$ for each $i\in S$, with
\[
	v_S = \frac{\theta c
		(1-\sched(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\sched(S))^c)^2}\enspace.
\]
The reducer for the key $i$ receives the pairs $(i,v_S)$ for each $S\in\Sample$
such that $i\in S$, and aggregates them to output the pair $(i,g_i)$, with
\[
g_i=\sched_i\sum v_S = g_i W_i\enspace.
\]
The set of pairs $(i, g_i)$, $1\le i\le n$ constitutes the input to the
next round which is just a parallel sum of all these values to obtain
\[
	g = \sum_{i=1}^n g_i = \sum_{i=1}^n\sched_iW_i
\]
This can be done in a single round thanks to the use of \emph{combiners}, i.e.,
partial sums computed after the map phase of the round. Finally in the third
round, the value $g$ is used in the $\mathsf{map}_3$ function applied to each
pair $(i, g_i)$ to compute the new value for $\sched_i = g_1 / g$, and output
$(i,\sched_i)$. In this third round the reducers just output the same pair
received in input, after which the new schedule is distributed to all machines
again and a new iteration can start.

The same results we had for the quality of the final schedule computed by
\algonameapx in case of convergence carry over to \algonamemr.
