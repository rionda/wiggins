\section{Computing a Schedule}\label{sec:method}
In this section, we study the problem of finding an optimal schedule in a
$(\theta,c)$-PHSP for a generating process $\sys = (V, \F, \pi)$ on a graph
$G=(V,E)$.

We start we start by assuming that we have complete knowledge of $\F$ and $\pi$.
This strong assumption allows us to study the theoretical properties of the
problem and motivates the design of our algorithm to compute an optimal
schedule. We then remove the assumption and show how we can extend our method
to only use a collection of observations from $\sys$ collected over a time
interval of length $O(\log n)$. Then we discuss how to recalibrate our
algorithms when the parameters of the process (e.g., $\pi$ or $\F$) change.
Finally, we show an algorithm for the MapReduce framework that allows us to
scale to large networks.

\subsection{Computing a Schedule}\label{sec:optimize}
Let $G=(V,E)$ be a graph. We study the $(\theta,c)$-PHSP for a generating
process $\sys=(V, \F, \pi)$. Assume for now that we know $\F$, and $\pi$. With
this assumption, we can exactly compute the $\theta$-cost of a $c$-schedule.

\begin{lemma}\label{lem:explicit}
For a schedule $\p$ we have
\begin{equation}\label{eq:explicit}
\cost{\p} \defeq
\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_{\sys}(t')] = \sum_{S\in \F} \frac{\pi(S)}{1- \theta(1-\p(S))^c},
\end{equation}
where $\p(S) = \sum_{v\in S} \p(v)$.
\end{lemma}

\begin{proof}
	Let $i=(t, S)$ be an item. The probability that $i$ is still uncaught at
	time $t'$ is $(1-\p(S))^{c(t'-t)}$, and its freshness at $t'$ is $\theta^{t'
	- t}$.  Therefore, $i$ imposes the load $\theta^{t'-t}$ to the system at
	time $t'$, with probability $(1-\p(S))^{c(t'-t)}$, and zero otherwise. Since
	the probability of $i_{t,S}$ being generated is $\pi(S)$ we have 
	\mynote{In my opinion, we need more passages in the following proof. E.g.,
		decomposing the expectation into a sum over all the timesteps $t\le t'$
		of sum over all sets $S$ in $\F$ of the probability of being generated
		at time $t$ and uncaught at time $t'$.}
	\begin{align*}
		\cost{\p} &=
		\lim_{t\rightarrow\infty}\frac{1}{t}\sum_{t'=0}^t\expect[L_{\sys}(t')] =
		\lim_{t\rightarrow \infty} \expect[L_{\sys}(t)] \\
		&=\lim_{t\rightarrow \infty} \sum_{S\in \F} \pi(S) (1 +
		\theta\cdot(1-\p(S))^c + \ldots + \theta^t\cdot(1-\p(S))^{ct}) \\ 
		&=
		\sum_{S\in \F} \frac{\pi(S)}{1-\theta(1-\p(S))^c},
		\end{align*}
		where we used the fact that for any convergent sequence
		$\set{a_i}_{i\in\mathbb{N}}$ we have
		$\lim\limits_{t\rightarrow\infty}\frac{\sum_{i=1}^t a_i}{t} =
		\lim\limits_{i\rightarrow\infty}a_i$ (Cesaro
		means~\cite{hardy1991divergent} \itodo{Is there a specific theorem we can
		cite?}).
\end{proof}

We now show that $\cost{\p}$, as expressed by the r.h.s.~of~\eqref{eq:explicit}
is a convex function over its domain, i.e., over the set of all possible
probability distribution over the set $V$ of nodes. We then use this result to
show how to compute an optimal schedule.

\begin{theorem}\label{thm:convexity}
	The cost function $\cost{\p}$ is a convex function over its domain.
\end{theorem}
\begin{proof}
	For any $S\in\F$, let $f_S(\p) = \frac{1}{1 - \theta(1-\p(S))^c}$. The
	function $\cost{\p}$ is a linear combination of $f_S(\p)$'s with positive
	coefficients. Hence to show that $\cost{\p}$ is convex it is sufficient to
	show that, for any $S\in\F$, $f_S(\p)$ is convex.

	We start by showing that $g_S(\p) = \theta(1-\p(S))^c$ is convex, because
	its Hessian matrix is positive semidefinite. Indeed we have:
	\[
		\frac{\partial}{\partial p_i \partial p_j} g_S(\p) = \left\{
		\begin{array}{lr}
		\theta c(c-1) (1-\p(S))^{c-2} &  i,j \in S\\
		0 &  \text{otherwise}
		\end{array}
		\right.
	\]
	Let $\mathbf{v}_S$ be a $n\times 1$ vector in $\mathbb{R}^n$ such that its
	$i$-th coordinate is $\left[c(c-1) (1-\p(S))^{c-2} \right]^{1/2}$ if $i\in
	S$, and $0$ otherwise. We can write the Hessian matrix of $g_S$ as
	\[
		\nabla^2 g_S = V_S * V_S^\mathsf{T},
	\]
	and thus, $\nabla^2 g_S$ is positive semidefinite matrix and $g$ is convex.
	From here, we have that $1-g_S$ is a \emph{concave} function. Since
	$f_S(\p)=\frac{1}{1-g_S(\p)}$ and the function $h(x)=\frac{1}{x}$ is convex
	and non-increasing, then $f_S$ is a convex function.
	\end{proof}
In the above proof, if for every $v\in V$, $S=\set{v}$ belongs to $\F$, then the
function $g_S$ is \emph{strictly} convex, and so is $f_S$. We then have the
following corollary of Thm.~\ref{thm:convexity}.

\begin{corollary}
	A schedule $\p$ with locally minimum cost, has also a global minimum cost
	and is an optimal schedule. Furthermore, if for every $v\in V$, $\set{v}$
	belongs to $\F$, the optimal schedule is \emph{unique}.
\end{corollary}

Finding the optimal schedule is then equivalent to solve the following
constrained convex optimization problem:
\begin{equation}\label{eq:optimization}
	\boxed{
	\begin{array}{rrclcl}
	\displaystyle \min_{\p} & \multicolumn{3}{l}{\cost{\p}} \\
	&\displaystyle \sum_{i=1}^{n} p_i & = & 1 \\
	&p_i & \geq & 0 & & \forall i \in \set{1,\ldots,n} \\
	\end{array} }
\end{equation}

Here, we present two approaches for solving this optimization problem: one by
using \emph{gradient descent} and one based on \emph{Lagrange multipliers}.
Using gradient descent, provides theoretical guarantee for converging to the
optimal schedule but is slow in practice for large networks. The other method is
fast but does not give a theoretical guarantee, though if it converges in finite
time to the final schedule, that schedule is provably optimal (see below).

\mynote{How is gradient descent handling the constraints? Also, the function has
	also a Hessian, so we could use Newton's method, which is even faster than
gradient descent}

\mynote{What is the real issue with gradient descent? Is it that we have to
	evaluate the cost function multiple times and this is computationally too
expensive. Is there a way to formalize this cost?}

\mynote{There are tons of methods for convex optimization. Why don't we care
about them?} 

\paragraph{Gradient Descent} As we showed in Theorem~\ref{thm:convexity}
the cost function is  (smoothly) convex. Thus, we can apply the gradient descent
method for the optimization in~\eqref{eq:optimization}, as converging to any
local minimum is guaranteed to give a global minimum.

\paragraph{Lagrange Multipliers: nonlinear iterative method} Generally, to use
the nonlinear iterative method, we need a function $Q:\triangle^{n-1}\rightarrow
\triangle^{n-1}$ such that $Q(\p^*) = \p^*$ for any optimal schedule $\p^*$.

\mynote{What is $\triangle^{n-1}$ ?}

\mynote{Is the following correct?}
From the method of Lagrance multipliers, we have that,
if a schedule $\p^*$ is optimal, then $p^*$ is a solution to following system of
$n$ equations:
\begin{equation}\label{eq:lagrange}
	\nabla [\cost{\p} + \lambda (p_1+\ldots+p_n)] = 0,
\end{equation}
\mynote{Shouldn't it be $p_1\ldots+p_n-1$ ?}
where the gradient on the l.h.s.~(which has $n+1$ components) is taken with
respect to (the components of) $\p$ and to $\lambda$.

For $1\le i\le n$, the $i$-th equation induced by~\eqref{eq:lagrange} is 
\[
	\frac{\partial}{\partial p_i} \cost{\p} + \lambda = 0,
\]
or, equivalently,
\[
	\sum_{S:i\in S} \frac{\theta c \pi(S)
		(1-\p(S))^{c-1}}{(1-\theta(1-\p(S))^c)^2} = \lambda\enspace.
\]
For notational convenience let 
\[
	W_i(\p) = \frac{\theta c \pi(S)
		(1-\p(S))^{c-1}}{(1-\theta(1-\p(S))^c)^2}
\]
\mynote{Is there supposed to be a sum in front above?}

and let $f: \mathbb{R} \rightarrow \mathbb{R}$ be an \emph{invertible} function.
We define $Q(\p) = (Q_1(\p), \ldots, Q_n(\p))$ as the vector whose $i$-th
component $Q_i(\p)$ is
\[
	Q_i(\p) = \frac{p_i f(W_i(\p))}{\sum_{j} p_j f(W_j(\p))}\enspace.
\]
We have
\[
	Q_i(\p^*) = \frac{p^*_i\cdot \lambda}{\sum_j p^*_j\cdot \lambda} =
p^*_i,\]
\mynote{I don't understand the above. Should it be $f(\lambda)$?}

and thus $Q(\p^*) = \p^*$. 

There is no guarantee that the method converges,  
\mynote{Which method? There has been no mention or description of an iterative
method yet.}

but we have that $Q(\p) = \p$ if and only if $\p$ is optimal, as shown by
Thm.~\ref{thm:iff}. In Sect.~\ref{sec:experiments} we show our experimental
results illustrating the convergence of this method in different cases.
\mynote{Do we actually have these?}

\begin{theorem}\label{thm:iff}
	If $\p$ is a schedule, then $Q(\p) = \p$ if and only if $\p$ is an optimal
	schedule.
\end{theorem}

\begin{proof}
	As shown above, if $\p$ is an optimal schedule $Q(\p)=\p$ holds.
	\mynote{Wouldn't it be better to have the proof here and only the
		description of the method (currently missing, in my opinion) outside the
	theorem?}
	
	Now assume that $Q(\p)=\p$. Therefore $p_i = Q_i(\p)$ for $1\leq i \leq n$,
	and we have
	\begin{align*}
	Q_i(\p) = p_i = \frac{p_i f(W_i(\p))}{\sum_{j} p_j f(W_j(\p))} 
	\Rightarrow
	W_i(\p) = f^{-1}\left(\sum_{j} p_j f(W_j(\p))\right).
	\end{align*}
	Hence, $W_i(p)$ is a constant (independent of $i$) 

	\mynote{I don't see the above.}
	
	and we have
	$W_1(\p)=\ldots=W_n(\p)$. Now, by letting $\lambda = W_i(\p)$, we obtain a
	solution for the system of equations in Lagrange multipliers method, and
	thus, $\p$ is an optimal schedule.
\end{proof}

In this work, we apply the second method as it is very fast on large datasets
that we use in our experiments. Also, although the convergence of this method is
not guaranteed, we show that in practice it converges quickly. For sake of
simplicity we took the identity function for $f$, and our method for the
optimization problem in~\eqref{eq:optimization} is given in
Algorithm~\ref{alg:iterative}.


\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKwComment{tcp}{//}{}
	\Input{Number $iter$ of iterations. XXX what about other?}
	\Output{XXX}
	$\p \leftarrow (1/n, \ldots, 1/n)$\;
	\For{$i\in\set{1,\ldots, \texttt{iterations}}$}{
		$W \leftarrow (0,\ldots, 0)$\;
		\For{$S\in \F$} {
			\For{$u \in S$} {
				$W_i \leftarrow W_i - \frac{\theta c \pi(S) (1-\p(S))^{c-1}}{(1-\theta(1-\p(S))^c)^2}$\;%\frac{-\theta\pi(S)}{(1-\theta(1-p(S)))^2}$\;
			}
		}
		$\p \leftarrow \frac{1}{\sum_{i} p_iW_i} (p_1W_1, \ldots, p_nW_n)$\;
	}
	\Return{$\p$}\;
	\caption{$\optimizer(\texttt{iterations})$}
	\label{alg:iterative}
\end{algorithm}  
	%However, this iterative method is convergent if the Jacobian norm of $F$ is less than 1, i.e., $\|DF\| < 1$. 

\subsection{Cost Approximation through Sampling}\label{sec:sampcomp}
	In a generating process $\sys = (U, \F, \pi)$, computing the cost of a schedule can be very challenging due to the parameters $\F$ and $\pi$ as they might not be known to us.
	Instead we have access to sampled {\ins} from $\F$ during the time. In particular, at any time step an {\ins} $S\in\F$ may be generated, and observed or sampled, with probability $\pi(S)$. A sample $\Sample$ of {\ins} during a time interval $[t_1, t_2]$ is $\Sample=\Sample_{t_1}\cup\ldots\cup\Sample_{t_2}$ where each $\Sample_t$ is the set of all observed {\ins} at time $t$.

	In this section, we study the optimization problem~(\ref{eq:optimization}) in generating processes for which we only have access to samples of $\F$. Specifically, we show that a sample $\Sample$ gathered during a time interval of length $\tilde O(\log(n))$ suffices to estimate and optimize the cost function.

	% In this section, we show how using a sample of sets in $\F$, formed during a short time interval of length $\tilde O(\log(n))$ we can estimate the $\cost{\p}$, $\nabla\cost{\p}$, and $W(\p)$ for any schedule $\p$ (during the optimization) accurately.  This shows the feasibility and accuracy of the Gradient Descent and the iterative methods when we have access to a sample of $\F$.

	We start by defining the cost of a schedule according to a sample:
	\begin{definition}[Cost according to a sample]
	Suppose $\p$ is a $c$-schedule and $\Sample$ is a sample of {\ins}s gathered during a time interval of length $\ell$. The cost of $\p$ according to $\Sample$, denoted by $\cost{\p,\Sample}$, is
	$\frac{1}{\ell}\sum_{S\in \Sample} \frac{1}{1-\theta(1-\p(S))^c}$.
	\end{definition}
	For a sample $\Sample$, $\cost{\p,\Sample}$ is an approximation of $\cost{\p}$, and the larger the size of $\Sample$ is, by the Law of Large Numbers we get better approximation. Lets denote the length of the time interval from which the sample $\Sample$ is obtained by $\ell(\Sample)$. The question we want to answer in this section is ``How large should $\ell(\Sample)$ be, so that for every schedule $\p$ we have $|\cost{\p,\Sample} - \cost{\p}| < \epsilon\cdot\cost{\p}$ w.h.p for $\epsilon > 0$?''

	We have the following theorem.
	\begin{theorem}\label{thm:chernoffcost}
	Suppose $\Sample$ is a sample gathered during a time interval of length $\ell(\Sample)
	\geq \frac{3(r\log(n)+\log(2))}{\epsilon^2(1-\theta)} =
	O\left(\frac{\log(n)}{\epsilon^2}\right)$. Then, for every schedule $\p$ we have
	$$\Pr(|\cost{\p,\Sample} - \cost{\p}|\geq \epsilon\cdot\cost{\p}) < \frac{1}{n^r}.$$
	\end{theorem}
	\begin{proof}
	Let $X_S$ be a random variable which is $\frac{1}{1-\theta(1-\p(S))^c}$ with probability $\pi(S)$, and zero otherwise. So, $1\leq X_S \leq \frac{1}{1-\theta}$. Also let  $X = \sum_{S\in \F} X_S$, and thus, 
	\begin{equation}\label{eq:boundcost}
		\cost{\p} = \expect[X] = \sum_{S\in\F} \expect[X_{S}] \geq |\F|, 
\end{equation}
and $|\F| \leq X \leq \frac{|\F|}{1-\theta}$. Let $X^i_S$ be the $i$-th draw of $X_S$ during the time interval $\Sample$ was sampled from and define $X^i = \sum_{S\in\F}X_S^i$. Note that 
$\cost{\p,\Sample} = \frac{1}{\ell(\Sample)}\sum_{i}X^i$. Now, denote $\mu=\frac{\ell(\Sample)(1-\theta)}{|\F|}\cost{\p}$. Using the fact that $\frac{1-\theta}{|\F|}X \in [0,1]$ and the Chernoff bound we have
\begin{align*}
\Pr\paran{\left|\cost{\p,\Sample} - \cost{\p}\right| \geq \epsilon \cdot \cost{\p}} 
&= 
\Pr\paran{\left|\sum_i X^i - \ell(\Sample)\cost{\p}\right| \geq \epsilon \ell(\Sample) \cost{\p}} \\
&=
\Pr\paran{\left|\frac{1-\theta}{|\F|}\sum_i X^i - \mu\right| \geq \epsilon \mu} \\
&\leq 2\exp\paran{-\frac{\epsilon^2\mu}{3}}\\
% \Pr\paran{\left|\frac{1-\theta}{|\F|}\sum_i X^i - \frac{\ell(\Sample)(1-\theta)}{|\F|}\cost{\p}\right| \geq \epsilon \frac{\ell(\Sample)(1-\theta)}{|\F|}\cost{\p}} \\
&=
2\exp\paran{-\frac{\epsilon^2 \ell(\Sample) (1-\theta) \cost{\p}}{3|\F|}} \\
&\leq 
2\exp\paran{-\frac{\epsilon^2 \ell(\Sample) (1-\theta)}{3}},
\end{align*}
where the last inequality holds since $\cost{\p}\geq |\F|$, as shown in Inequality (\ref{eq:boundcost}).
So, if $\ell(\Sample) \geq \frac{3 (\ell \log(n)+\log(2))}{\epsilon^2(1-\theta)} = O\paran{\log(n)/\epsilon^2}$, for every schedule $\p$, $\left|\cost{\p,\Sample} - \cost{\p}\right| < \epsilon \cdot \cost{\p}$ with probability at least $1-1/n^r$.
\qed
\end{proof}

In the previous section we defined the $W_i(\p)$ functions on schedules for a given generating process $\sys=(U,\F,\pi)$, assuming we know $\pi$. However, similar to the cost function, we can define these functions according to a sample, namely, 
$W_i(\p,\Sample) = \frac{1}{\ell(\Sample)}\sum_{S\in \Sample: i\in S} \frac{\theta c (1-\p(S))^{c-1}}{(1-\theta(1-\p(S))^c)^2}$.
% $W_i\frac{\theta c \pi(S) (1-\p(S))^{c-1}}{(1-\theta(1-\p(S))^c)^2}$. 
Using a very similar argument we have the following theorem:
\begin{theorem}\label{thm:chernoffW}
 Suppose $\Sample$ is a sample gathered during a time interval of length $\ell(\Sample) \geq \frac{3(r\log(n)+\log(2))}{\epsilon^2(1-\theta)} = O\paran{\frac{\log(n)}{\epsilon^2}}$. Then, for every schedule $\p$ and $i\in\set{1,\dots,n}$ we have
 $$\Pr(|W_i(\p,\Sample) - W_i(\p)|\geq \epsilon\cdot W_i(\p)) < \frac{1}{n^r}.$$
\end{theorem}

We conclude this section by providing an algorithm, Algorithm~\ref{alg:iterative_with_sample}, for the optimization problem (\ref{eq:optimization}) in the case that we only have access to a sample of $\F$. Note that by Theorems~\ref{thm:chernoffcost} and~\ref{thm:chernoffW}, in the iterative process of Algorithm~\ref{alg:iterative_with_sample}, our estimates of the cost and $W_i$'s function are within an $\epsilon$ factor of their true value, with high probability (using an appropriate $r$).

\begin{algorithm}[!h]
\BlankLine
{\bf Inputs:} The number of  $\texttt{iterations}$, and a sample of {\ins}s $\Sample$.

\Begin{
$\p \leftarrow \paran{1/n, \ldots, 1/n}$\;
\For{$i\in\set{1,\ldots, \texttt{iterations}}$}{
	$W \leftarrow (0,\ldots, 0)$\;
	\For{$S\in \F$} {
		\For{$u \in S$} {
			$W_i \leftarrow W_i - \frac{\theta c (1-\p(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\p(S))^c)^2}$\;%\frac{1}{\ell(S)\cdot(1-\theta(1-p(S)))^2}$\;
		}
	}
	$\p \leftarrow \frac{1}{\sum_{i} p_iW_i} (p_1W_1, \ldots, p_nW_n)$\;
}
  return $\p$\;
}
\caption{$\appoptimizer(\texttt{iterations},\Sample)$}\label{alg:iterative_with_sample}
\end{algorithm}  







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Learning Dynamic Parameters}\label{sec:dynamic}
In this short section, we consider the case when the parameters of the network change (i.e, the distribution over the possible informed-sets changes).  To apply our method to a dynamic environment we first sample the process to generate a sufficiently large sample collection. This can be done by probing
 all nodes with uniform distribution during a short time interval, or using a round robin schedule (Algorithm~\ref{alg:sampler}). For an item the sampling process stores the set of nodes that received this item. We then compute an optimal schedule with respect to that sample (Algorithm~\ref{alg:iterative}). We use this schedule and monitor the cost to detect significant changes. In that case we obtain a new sample, optimize with respect to that sample and apply the new schedule. 
 
 Note that when we adapt our schedule to the new environment (using the most recent sample) the system converges to its stable setting exponentially (in $\theta$) fast: Suppose $L$ items have been generated since the change of the parameters until we adapt the new schedule. These items, if not caught, loose their freshness exponentially fast: after $t$ steps their freshness is at most $L\theta^t$ and gets diminished very quickly. 
 
 In our experiments we provide different examples that illustrate how load of the generating process stables after the algorithm adapts itself to the changes of parameters (see Section\ref{sec:exp}).
 
   
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[!h]
\BlankLine
{\bf Inputs:} The length of the time interval that we sample from, $\texttt{time\_length}$.

\Begin{
$\D_1 \leftarrow \emptyset$\;
$\D_2 \leftarrow \emptyset$\;
 \For{nodes $u\in U$}{
   \For{items $i$ at $u$, not older than \texttt{time\_length} time steps}{
      $\D_1 \leftarrow \D_1 \cup \set{i}$\;
      $\D_2 \leftarrow \D_2 \cup \set{(i,u)}$\;
      }
    }
    
  
$\Sample \leftarrow \emptyset$\;
\For{$i \in D_1$}{
  $S \leftarrow \emptyset$\;
  \For{$(i, u) \in D_2$}{
  $S \leftarrow S \cup \set{u}$\;
  }
  $\Sample \leftarrow \Sample \cup \set{S}$\;
  }
  
  return $\Sample$\;

}

\caption{$\sampler(\texttt{time\_length})$}\label{alg:sampler}
\end{algorithm}  
   
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Optimizer %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}[!h]
% \BlankLine
% {\bf Inputs:} A sample collection, $\M$, and the number of iterations, $\texttt{iter}$.
% 
% \Begin{
% $\p\leftarrow (1/n,1/n,\ldots,1/n)$\;
%  \For{$t\in\set{1,\ldots,\texttt{iter}}$}{
%   $c\leftarrow (0,\ldots,0)$\;
%    \For{$S \in \M$}{
%      \For{$i \in \Sample$}{
%       $c_i \leftarrow c_i + \paran{\frac{p_i}{\sum_{j\in S p_j}}}^2$ \;
%       }
%     }
%     
%   }
% \For{$i\in \set{1,\ldots,n}$}{
%   $p_i \leftarrow \frac{1}{\sum_{i=1}^n c_i}(c_1, \ldots, c_n)$\;
%   }
% return $\p$\;
% }
% 
% \caption{$\optimizer(\M, \texttt{iter})$}\label{alg:optimizer}
% \end{algorithm}









%  Outline of the algorithm is given in Figure~\ref{??}.

% \subsection{Sufficient sample}
% Our goal is to compute an (approximately) optimal schedule using one sample collection 
% ${\cal M}=\{S_1,\dots,S_m\}$, of $m$ informed sets generated according to the distribution $\pi$.  
% Identifying an optimal function among a set of functions using one sample collection requires strong statistical properties of the sample.
% We need to show that the cost of {\em any} schedule $\p$, considered though out the optimization process, on the sample is a close approximation of the cost of $\p$ on the entire distribution $\pi$. 
% In particular we require that for very $i=1,\dots,n$, and any schedule $\p$, the estimate
% $${C}_i(\p,\M)=\frac{1}{m}\sum_{i=1}^m \left( \frac{p_i}{\sum_{j\in S_i} p_j} \right )^2$$ is sufficiently good estimate for
% $${C}_i(\p)=\sum_{S:i\in S} \pi(S) \left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2.$$
% Thus, we need a {\em uniform convergence result} of the sample with respect to this functions over all possible schedules.
% 
% We use the {\em Rademacher complexity} method~\cite{shalev2014understanding} to analyze our sample:
% Given a set of functions $F$ and a set of $m$ samples $M=z_1,\dots,z_m$, the
% %Rademacher complexity $R_m(F)$ and the 
% empirical Rademacher sum $R_M(F)$ 
% %are 
% %is $$R_m(F) = E_S [E_{\bar{\sigma}} [\sup_{f \in F}  ( \frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i )]], $$
% is $$ R_M (F) = E_{\bar{\sigma}} \left[\sup_{f \in F} \paran{\frac{1}{m} \sum_{i=1}^m f(z_i) \sigma_i} \right],$$
% %respectively, 
% where $\bar{\sigma}=(\sigma_1,\dots, \sigma_m)$ is a vector of $m$ independent random variables, with $Pr(\sigma_i =1)=Pr(\sigma_i = -1)=1/2$. 
% %The role of the $\sigma$ variables is to create a kind of output noise in the dataset. The supremum carries out an optimization process over all functions in %$F$ trying to fit this noise. The Rademacher complexity is high if the function class is successful at fitting this noise and low otherwise.
% Like the VC-dimension (for range spaces), the Rademacher sum is a measure of the expressiveness or complexity of a set of functions $F$. We use the following relation between the Rademacher sum and the uniform convergence of a sample:
% 
% \begin{theorem}~\cite[Theorem 3.1]{mohri2012foundations} 
% \label{th:rc}
% Let $F$ be a (finite or measurable) set of functions on a domain $Z$ such that for all $f\in F$, $f:Z\rightarrow [0,1]$. Let  $M=\{z_1,\dots, z_m\}$ be a sample chosen from a distribution $D$ on $Z$. 
% With probability $1-\delta$ over the choices of $M$,
% %over the distribution of $S$, 
% simultaneously for all functions $f\in F$, we have
% %$$E_D[f(z)] \le E_S[f(z)] + 2 R_m(F) + \sqrt{\frac{\ln(1/\delta)}{2m}},$$
% %and
% %$$ E_D[f(z)] \le E_S[f(z)] + 2 R_S(F) + 3\sqrt{\frac{\ln(2/\delta)}{2m}}.$$
% \begin{eqnarray}
% \label{bound1}
% E_D[f(z)] \le \hat{E}_M [f(z)] + 2 R_M (F) + 3\sqrt{\frac{\ln(1/\delta)}{2m}},
% \end{eqnarray}
% \end{theorem}
% 
% To apply this theorem to our problem we observe that 
% $$\frac{{C}_i(\p)}{ \sum_{S:i\in S} \pi(S) }=\frac{1}{ \sum_{S:i\in S} \pi(S) }\sum_{S:i\in S} \pi(S) \left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2$$
% is the expectation of the function $f^i_{\p}(S)=\left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2$ over the distribution $D_{|i}$ (the distribution of sets that include node $i$). To bound the Rademacher empirical sum of the set of functions
% $$F=\left \{f^i_{\p}(S)=\left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2~\big |~1\leq i\leq n,~\p\in [0,1]^n, ~\sum_{i=1}^n p_i=1\right \}$$
% we consider a discrete version of this family of functions, $DF$, where
% $p_i\in\{n^{-2}(1+\frac{1}{8\log n})^k,~0\leq k\leq  16\log^2 n\}$ and 
% $\sum_{i=1}^n p_i \leq 1+\frac{1}{8\log n}.$ Thus, each $p_i$ is restricted to $16\log^2 n +1$ values, and rounding to this set of values increases the value of the functions we consider by no more than 
%  $\left (\frac{\frac{1}{n^2} +(1+\frac{1}{8\log n})}{1-\frac{1}{8\log n}}\right )^2 \leq (1+\frac{1}{\log n})$.
% 
% We apply Massart Lemma~\cite[Lemma 26.8]{shalev2014understanding} (adapted to our case):
% \begin{lemma}
% Let $DF=\{f_1,\dots, f_N\}$ be a set of $N$ functions, and let $x_1, \dots, x_m$ be $m$ samples. 
% Let $$R=\max_{f\in F} \paran{\sum_{i=1}^m f(x_i)^2}^{1/2}$$ then
% $$R_m(F)\leq \frac{R\sqrt{2\log N}}{m}.$$
% \end{lemma}
% The number of functions in our case is $N=n(16\log n+1)^n$, and $R\leq \sqrt{m}$, since for any $\p$ and $S$,
% $\left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2 \leq 1$.
% Thus,
% $$R_m(F)\leq (1+\frac{1}{\log n}) R_m(DF)\leq (1+\frac{1}{\log n})\sqrt{\frac{{4n\log\log n}}{{m}}}.$$
% 
% Applying Theorem~\ref{th:rc} we prove that when using a sample collection of size $m=4n\log n \log\log n$, with probability $1-\frac{1}{n}$, we approximate all the functions within an additive
%  factor of $\frac{1}{\log n}$.
%  
%  Our analysis so far did not assume any particular propagation structure on the network. However, in novelty discovery in a social network we are mostly interested in discovering items that are distributed within small communities and can ignore items that are already distributed to a large fraction of the network.  Assume that the communities we are mostly interested in have size $\alpha(n)<<n$, then the number of functions in the corresponding set $DF$ is
%  bounded by $N=n(16\log n+1)^\alpha(n)$ and the corresponding sample size needed for the discovery is reduced by a multiplicative factor of $\alpha(n)/n$.
% 
% 
\subsection{Scaling up: MapReduce Algorithm}\label{sec:mapreduce}
In this section, we show the scalability of {\optimizer} and {\appoptimizer} (see Algorithms~\ref{alg:iterative} and \ref{alg:iterative_with_sample}) in MapReduce framework \cite{dean2008mapreduce}. A MapReduce model consists of two parts: mapping by \texttt{mappers}, and reducing by \texttt{reducers}. At each round of MapReduce the input dataset is partitioned into independent chunks and each chunk is sent to a \texttt{mapper}. Each mapper will process the received chunk and outputs pairs of key-values, $\langle k, v \rangle$. Then, the  platform shuffles the key-value pairs and aggregate the values of a same key and sends them to \texttt{reducers}. Therefore, each \texttt{reducer} receives inputs of form $\langle k, (v_1, \ldots, v_r) \rangle$, where $v_1, \ldots, v_r$ are all the values with the key $k$ generated by \texttt{mappers}. Finally, \texttt{reducers} process their inputs and return the final outputs.

In following, we show how computing the cost and $W_i$ functions (and consequently $Q$)  can be done in a MapReduce given a sample $\Sample$ for a generating process $\sys=(U,\F,\pi)$. Therefore, {\appoptimizer} can be easily scaled to larger dataset.
%for a generating process $\sys=(U,\F,\pi)$, with  full access to $\F$ and $\pi$. In a very similar argument, the method extends to the case when we only have access to a sample  $\Sample$ of $\F$ and we have to compute the cost and $W_i$ functions according to the sample $\Sample$. 
So, lets assume the input dataset is $\Sample$, and the {\ins} in $\Sample$ are partitioned and sent to \texttt{mappers}. We also assume that \texttt{mappers} have a copy of $\p$. 


\paragraph{\bf Computing the cost function.}
Each \texttt{mappers}, for every $S\in \Sample$ that it receives, outputs the following key-value pair:
$\left\langle 0,  \frac{1}{\ell(\Sample)}f(S) \right\rangle,$
where $f(S) = \frac{1}{1- \theta(1-p(S))^c}$.
Next, a \texttt{reducer} receives an input of the form 
$\left\langle 0,  \left[\frac{1}{\ell(\Sample)}f(S)\right]_{S\in \Sample}  \right\rangle,$
and outputs $\frac{\sum_{S\in \Sample}f(S)}{\ell(\Sample)}$.

\paragraph{\bf Computing  the $W_i$'s functions.}
Each \texttt{mappers}, for every $S\in \Sample$ that it receives, outputs all the following key-value pairs:
$$\left\langle i,  \frac{-\theta c (1-\p(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\p(S))^c)^2} \right\rangle, \forall i \in S.$$
Next, each \texttt{reducer} receives an input of the form
$$\left\langle i,  \left[\frac{-\theta c (1-\p(S))^{c-1}}{\ell(\Sample)(1-\theta(1-\p(S))^c)^2}\right]_{S\in \Sample: i\in S} \right\rangle,$$
and would be able to compute $p_iW_i(\p, \Sample)$, by adding the ``values'' and multiply by $p_i$.
% $$p_iW_i(\p) = \sum_{S: i\in S}\frac{-\theta}{\ell(\Sample)\paran{1-\theta(1-\p(S))}^2}.$$ 
Finally, the aggregator (or the central processing unit) can normalize the vector $(p_1W_1(\p), \ldots, p_nW_n(\p))$, and compute $Q(\p)$.

Using a very similar technique, one can compute the cost and $W_i$'s functions in MapReduce framework when having full access to $\F$ and $\pi$ parameters of the generating process $\sys=(U,\F,\pi)$. This shows the scalability of {\optimizer}.
%Each \texttt{mappers}, for every $S\in \Sample'$ that it receives, outputs all the following key-value pairs:
%$$\left\langle i,  \frac{\theta^2 f(S)^2}{|\Sample'|} \right\rangle, \forall i \in S.$$
%Next, each \texttt{reducer} receives an input of the form
%$$\left\langle i,  \left[\frac{\theta^2 f(S)^2}{|\Sample'|}\right]_{S\in \Sample': i\in S} \right\rangle,$$
%and would be able to compute the $i$-th coordinate of the gradient:
%$$\frac{\partial}{\partial p_i} \cost{\p} = - \frac{\theta^2}{|\Sample'|}\sum_{S\in \Sample': i\in S} f(S)^2.$$

% Now, we show how each round of iteration in Algorithm~\ref{alg:optimizer} can be implemented in a MapReduce model: Note that the goal in each iteration is to compute 
% $$c_i = \sum_{S\in \M: i\in S} \paran{\frac{p_i}{\sum_{j\in S}p_j}}^2,$$
% for $1 \leq i \leq n$. The input dataset is $\M$, and the informed sets in $\M$ are partitioned and sent to \texttt{mappers}. We also assume that \texttt{mappers} have a copy of $\p$. Each \texttt{mapper} for any $S\in\M$ that it receives, outputs all the following key-value pairs:
% $$\langle i,  (\paran{p_i, p(S)})\rangle, \forall i \in S,$$
% where $p(S) = \sum_{j\in S} p_j$ (note that each value is also an ordered pair). 
% Next, each \texttt{reducer} receives inputs of form $\langle i, [(p_i, p(S_1)), \ldots, (p_i, p(S_r))]\rangle$
% where $S_1, \ldots, S_r$ are all the informed set in $\M$ that include the node $i$, and outputs 
% $$c_i = \sum_{t=1}^r \paran{\frac{p_i}{\sum_{j\in S_t} p_j}}^2,$$
% which is the result of one iteration in Algorithm~\ref{alg:optimizer}. 
% 
% Note that the running time of $\optimizer$ is $O(|\M|)$ (linear) given a constant number of iterations. In particular, if $|\M| = O(|V|+|E|)$ the whole process of sampling and optimizing is linear in the network size, where $V$ and $E$ are the nodes and edges sets of the network, respectively.
% 
% Given a schedule $\p=(p_1,\dots,p_n)$ the expected time to probe at least one node in a set $S$ is $\frac{1}{\sum_{j\in S}p_j} $. Since $\pi(S)$ is the probability of generating an item that is distributed to an informed set $S$, the cost of schedule $\p$ is
% $\cost{\p} =\sum_{S\subseteq V} \frac{\pi(S)}{\sum_{j\in S} p_j},$ and our goal is to compute an optimal schedule $\p^*=\arg\min_{\p} \cost{\p,\pi}$. There are two main difficulties in computing the optimal schedule: (1) the number of terms in the expression for the cost is exponential in $n$; and (2) we do not know that distribution $\pi$, and it can vary in time.
% 
% Our solution consists of two major components. We first prove, and demonstrate in simulations, that a relatively small sample size is sufficient to characterize
% a close to optimal schedule, we then show how to efficiently compute an optimal schedule with respect to the sample. 
% Both parts use the following derivation for the cost function. Intuitively, this expression
% ``distribute'' the cost of finding an item between the vertices of the set that receives this item (we give more intuition for using this form later).
% 
% \begin{eqnarray}
% \label{eq:cost}
% \cost{p}&= &\sum_{S\subseteq V} \frac{\pi(S)}{\sum_{j\in S} p_j} \nonumber \\ 
% &=& \sum_{i\in V} \sum_{S:i\in S} \frac{\pi(S)}{\sum_{j\in S} p_j} \frac{p_i}{\sum_{j\in S} p_j}  \nonumber 
% \\ &=&
%  \sum_{i\in V} \frac{1} {p_i}\sum_{S:i\in S} \pi(S) \left( \frac{p_i}{\sum_{j\in S} p_j} \right )^2 
%  \end{eqnarray}
