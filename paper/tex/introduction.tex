\section{Introduction}\label{sec:introduction}
An emerging trend in algorithmic stock trading is the use of automatic search through the Web, the blogosphere, and social networks for relevant information that can be used in fast trading, before it appears in the more popular news sites~\cite{Delaney2009,ALPHA2014,AlphaFlash,mitra2011handbook,latar2015robot,wallstreet2015,McKinney2011}. Similarly, intelligence, business and politics analysts are scanning online sources for new information or rumors. While new items are often reblogged, retweeted, and posted on a number of sites, the goal is to find the information once, as fast as possible, before it loses its relevance. There is no benefit in seeing more copies of the same news item, rumor, etc.

Such a search is an example of a fundamental search and detection problem in dynamic, distributed massive data repository.  Data is distributed among a large number of nodes, new items appear in individual nodes, and items may propagate (copied) to neighboring nodes on a physical or a virtual network.  The goal is to detect at least one copy of each new item as soon as possible. The search application can probe any node in the system, but it can only access and process a few nodes at a time. 
To minimize the time to find new items, the search application needs to optimize the schedule of probing nodes, taking into account (i) the  distribution of copies of items among the nodes (to see where to probe), and (ii) the decay of items' freshness (or relevance) over time (to focus the search on most relevant items).

% the likelihood of nodes to generate new items, to receive new items, to propagate items to other nodes, and how yet undiscovered items loose their impact/value over the time.

Another application that faces a similar search problem is processing security raw data (e.g. gathered security raw footages that need to be processed). Ideally, given limited resources compared to the huge volume of the data, a schedule for processing (probing) different raw-dataset has to minimize the value of missing information (assuming newer information are more valuable than older ones).

Our goal is to provide an efficient tool for a user who {\bf does not} have a prior information on the generation and distribution of items in the network. We focus on \emph{simple}, \emph{memoryless} probing schedules that are easy to compute and require minimum storage. In the next section, we formally introduce the model and define our problem.
%Our solution consists of two major components. We first develop an efficient algorithm for computing a near optimal schedule. Then, using a Chernoff bound inequality, we show prove having only to a family $\M$  of \emph{sampled} informed set gathered during a very short amount of time (during $O(\log(n))$ time intervals, where $n$ is the number of nodes), without having access to $\S$,  $\pi$, or the network topology, we are still able to apply our method accurately. }

\paragraph*{Contributions}
In this work we introduce the \probname, \itodo{One liner}.  Specifically, our
contributions are the following

\begin{itemize*}
	\item \todo[Matteo]{List our contributions.}
\end{itemize*}

\para{Paper Structure} In Sect.~\ref{sec:prelims} we give introductory
definitions and formulate the \probname, showing how it is related to
information-diffusion processes in a graph. In Sect.~\ref{sec:method} we
describe our algorithms for computing an optimal schedule and its cost
\itodo{verify that we define what a schedule is before this}, and then show
(Sect.~\refmissing) how to adapt the algorithm to the realistic case when the
parameters of the problem needs to be learned from a collection of observations
and the cost can only be estimated. Then, in Sect.~\refmissing we show a
MapReduce algorithm for the same problem which greatly speeds up the computation
and allows to scale up to large networks. The result of our experimental
evaluation are presented in Sect.~\ref{sec:exp}. We draw our conclusions and
outline potential directions for future work in Sect.~\ref{sec:concl}.
