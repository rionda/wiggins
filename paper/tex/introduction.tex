\section{Introduction}\label{sec:introduction}
Many applications require the detection of events in a network as soon as they
happen or shortly thereafter, as the value of the information obtained by
detecting the events decays rapidly as times passes. For example, an emerging
trend in algorithmic stock trading is the use of automatic search through the
Web and social networks for pieces of information that can be used in trading
decisions before they appear in the more popular news
sites~\citep{Delaney2009,ALPHA2014,AlphaFlash,mitra2011handbook,latar2015robot,wallstreet2015,McKinney2011}.
\mynote[Matteo]{Do we really want all these newspaper journals and software
	manuals in the references? I would love to cite the TwoSigma
	one~\citep{wallstreet2015}, obviously ;), but it seems a bit too much to
have 7 references for this?}
Similarly, intelligence, business and politics analysts are scanning online
sources for new information or rumors. While new items are often reblogged,
retweeted, and posted on a number of sites, it is sufficient to find an item
once, as fast as possible, before it loses its relevance. There is no benefit in
seeing more copies of the same news item, or rumor. A similar situation also
arises in the need to detecting intrusions, infections, or defects in,
respectively, a computer network, a public water system, or a large electronic
circuit.

Indeed this is a fundamental search and detection problem in a distributed data
setting, not limited to social networks or graph analysis. In this setting, the
data is distributed among a large number of nodes, and new items appear in
individual nodes (for example, as the products of processing the data available
locally at the node), and may propagate (being copied) to neighboring nodes on a
physical or a virtual network. The goal is to detect at least one copy of each
new item as soon as possible. The search application can access any node
in the system, but it can only \emph{probe} (i.e., check for new items on) a few
nodes at a time. To minimize the time to find new items, the search application
needs to optimize the schedule of probing nodes, taking into account (i) the
distribution of copies of items among the nodes (to see where to probe), and
(ii) the decay of items' \emph{freshness} (or relevance) over time (to focus the
search on most relevant items). The  main challenge is in devising a good
probing schedule in absence of prior knowledge on the generation and
distribution of items in the network.

% Commented by Matteo on 06/29
%Another application that faces a similar search problem is processing security
%raw data (e.g. gathered security raw footages that need to be processed).
%Ideally, given limited resources compared to the huge volume of the data, a
%schedule for processing (probing) different raw-dataset has to minimize the
%value of missing information (assuming newer information are more valuable than
%older ones).

\paragraph*{Contributions}
In this work we study the problem of computing an optimal node probing schedule
for detecting new items in a network under resource scarceness, i.e., when only
a few nodes can be probed at a time. Our contributions to the study of this
problem are the following:

\begin{itemize*}
	\item We formalize a generic process that describe the creation and
		distribution of information in a network and clearly define the task of
		studying this process by probing the nodes in the network according to a
		schedule. The process and task are parametrized by the resource
		limitations of the observer and the decay rate of the freshness of
		items. We then introduce a \emph{cost} measure to compare different
		schedules. The cost of a schedule is the limit of the average expected
		freshness of uncaught items at each time step.
		\todo{Mention the actual name of the problem we study.}
	\item We conduct a theoretical study of the cost of a schedule, showing that
		it can be computed explicitly and that is a convex function over the
		space of schedules. We then introduce \algoname,\footnote{In the Sherlock
			Holmes novel \emph{A study in scarlet} by A.~Conan Doyle, Wiggins is
			the leader of the ``Baker Street Irregulars'', a band of street
			urchins employed by Holmes as intelligence agents.} an algorithm to
		compute the optimal schedule by solving a constrained convex
		optimization problem.
		\todo{Mention iterative method through Lagrange multipliers.}
	\item We discuss variants of \algoname for the realistic situation where
		the parameters of the process needs to be learned or can change over
		time. Among these variants, we present an adaptation for the MapReduce
		framework of computation, which allow us to handle very large networks.
	\item Finally, we conduct an extensive experimental evaluation of \algoname
		and its variants, comparing the performances of the schedules it
		computes with natural baselines, and showing how it performs extremely
		well in practice on real social networks when using well-established
		models for generating new items (e.g., the independence cascade
		model~\citemissing).
\end{itemize*}

To the best of our knowledge, the problem we study is novel and we are the first
to devise an algorithm to compute an optimal schedule, both when the generating
process parameters are known and when they need to be learned. Many variants
of the problem we introduce can be devised but, due to lack of space, we defer
their study to the extended version of this work.

\para{Paper Organization} In Sect.~\ref{sec:prelims} we give introductory
definitions and formally introduce the settings and the problem. We discuss
related works in Sect.~\ref{sec:related_work}. In Sect.~\ref{sec:method} we
describe our algorithm \algoname and its variants. The results of our
experimental evaluation are presented in Sect.~\ref{sec:exp}. We conclude by
outlining directions for future work in Sect.~\ref{sec:concl}.
